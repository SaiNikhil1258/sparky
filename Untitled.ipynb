{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99d241a1-7d62-48d9-ac76-4f118243b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "import urllib.request\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.pandas.utils import spark_column_equals\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import builtins\n",
    "import random\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "python_path = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['JAVA_HOME'] = r'C:\\Users\\LENOVO\\.jdks\\corretto-1.8.0_422'\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\Program Files\\Hadoop\"\n",
    "\n",
    "conf = (SparkConf()\n",
    "\t\t.setAppName(\"pyspark\")\n",
    "\t\t.setMaster(\"local[*]\")\n",
    "\t\t.set(\"spark.driver.host\", \"localhost\")\n",
    "\t\t.set(\"spark.default.parallelism\", \"1\")\n",
    "\t\t)\n",
    "\n",
    "# Check if a SparkContext already exists\n",
    "if 'sc' in globals():\n",
    "    sc.stop()  # Stop the existing SparkContext  \n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4957d",
   "metadata": {},
   "source": [
    "1. Assume you're given a table Twitter tweet data, write a query to obtain a histogram of tweets posted per user in 2022.\n",
    " Output the tweet count per user as the bucket and the number of Twitter users who fall into that bucket.\n",
    " In other words, group the users by the number of tweets they posted in 2022 and count the number of users in each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93862ddc-676f-48b7-95b2-dec3d4ad88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+-------------------+\n",
      "|tweet_id|user_id|                 msg|         tweet_date|\n",
      "+--------+-------+--------------------+-------------------+\n",
      "|  214252|    111|Am considering ta...|12/30/2021 00:00:00|\n",
      "|  739252|    111|Despite the const...|01/01/2022 00:00:00|\n",
      "|  846402|    111|Following @NickSi...|02/14/2022 00:00:00|\n",
      "|  241425|    254|If the salary is ...|03/01/2022 00:00:00|\n",
      "|  231574|    148|I no longer have ...|03/23/2022 00:00:00|\n",
      "+--------+-------+--------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- msg: string (nullable = true)\n",
      " |-- tweet_date: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+\n",
      "|user_id|tweet_count_per_user|\n",
      "+-------+--------------------+\n",
      "|    111|                   2|\n",
      "|    148|                   1|\n",
      "|    254|                   1|\n",
      "+-------+--------------------+\n",
      "\n",
      "+--------------------+------------+\n",
      "|tweet_count_per_user|tweet_bucket|\n",
      "+--------------------+------------+\n",
      "|                   1|           2|\n",
      "|                   2|           1|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    "(\"214252\",\t\"111\",\"Am considering taking Tesla private at $420. Funding secured.\",\t    \"12/30/2021 00:00:00\"),\n",
    "(\"739252\",\t\"111\",\"Despite the constant negative press covfefe\",\t                     \"01/01/2022 00:00:00\"),\n",
    "(\"846402\",\t\"111\",\"Following @NickSinghTech on Twitter changed my life!\",                \"02/14/2022 00:00:00\"),\n",
    "(\"241425\",\t\"254\",\"If the salary is so competitive why won’t you tell me what it is?\",   \"03/01/2022 00:00:00\"),\n",
    "(\"231574\",\t\"148\",\"I no longer have a manager. I can't be managed\",                      \"03/23/2022 00:00:00\"),\n",
    "]\n",
    "df_daily_sales = spark.createDataFrame(x, ['tweet_id', 'user_id', 'msg', 'tweet_date'])\n",
    "\n",
    "df_daily_sales.show()\n",
    "df_daily_sales.printSchema()\n",
    "df_with_timestamp = (df_daily_sales\n",
    "                     .withColumn(\n",
    "                    \"year\",\n",
    "                        to_timestamp(df_daily_sales[\"tweet_date\"], \"MM/dd/yyyy HH:mm:ss\"))\n",
    "                    )\n",
    "# filtering the  tweets in the year 2022\n",
    "df = (df_with_timestamp\n",
    "        .filter(year(df_with_timestamp[\"year\"]) == 2022)\n",
    "        .groupby(\"user_id\")\n",
    "        .agg(count(\"tweet_id\").alias(\"tweet_count_per_user\")) )\n",
    "df.show()\n",
    "\n",
    "# grouping by the count of the tweets\n",
    "res = df.groupby(\"tweet_count_per_user\").agg(count(\"user_id\").alias(\"tweet_bucket\"))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df6fa9",
   "metadata": {},
   "source": [
    "\n",
    "2. Given a table of candidates and their skills, you're tasked with finding the candidates best\n",
    " suited for an open Data Science job. You want to find candidates who are proficient in Python, Tableau, and PostgreSQL.\n",
    "Write a query to list the candidates who possess all of the required skills for the job.\n",
    "Sort the output by candidate ID in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8abc3da-e59c-4e7e-a42b-1db88bb32c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|candidate_id|     skill|\n",
      "+------------+----------+\n",
      "|         123|    Python|\n",
      "|         123|   Tableau|\n",
      "|         123|PostgreSQL|\n",
      "|         234|         R|\n",
      "|         234|   PowerBI|\n",
      "|         234|SQL Server|\n",
      "|         345|    Python|\n",
      "|         345|   Tableau|\n",
      "+------------+----------+\n",
      "\n",
      "root\n",
      " |-- candidate_id: string (nullable = true)\n",
      " |-- skill: string (nullable = true)\n",
      "\n",
      "+------------+----------+\n",
      "|candidate_id|     skill|\n",
      "+------------+----------+\n",
      "|         123|    Python|\n",
      "|         123|   Tableau|\n",
      "|         123|PostgreSQL|\n",
      "|         345|    Python|\n",
      "|         345|   Tableau|\n",
      "+------------+----------+\n",
      "\n",
      "+------------+-----------+\n",
      "|candidate_id|skill_count|\n",
      "+------------+-----------+\n",
      "|         123|          3|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+--------------------+\n",
      "|candidate_id|              skills|\n",
      "+------------+--------------------+\n",
      "|         234|[PowerBI, R, SQL ...|\n",
      "|         345|   [Python, Tableau]|\n",
      "|         123|[PostgreSQL, Pyth...|\n",
      "+------------+--------------------+\n",
      "\n",
      "Column<'array(PostgreSQL, Python, Tableau)'>\n",
      "+------------+--------------------+\n",
      "|candidate_id|              skills|\n",
      "+------------+--------------------+\n",
      "|         123|[PostgreSQL, Pyth...|\n",
      "+------------+--------------------+\n",
      "\n",
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|         123|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"candidate_id\",\t\"skill\"]\n",
    "data = [\n",
    "    (\"123\",\t\"Python\"),\n",
    "    (\"123\",\t\"Tableau\"),\n",
    "    (\"123\",\t\"PostgreSQL\"),\n",
    "    (\"234\",\t\"R\"),\n",
    "    (\"234\",\t\"PowerBI\"),\n",
    "    (\"234\",\t\"SQL Server\"),\n",
    "    (\"345\",\t\"Python\"),\n",
    "    (\"345\",\t\"Tableau\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "# using dictionary to check for the skills needed\n",
    "required_skills_dict = {\"Python\", \"Tableau\", \"PostgreSQL\"}\n",
    "filtered_candidates = df.filter(df.skill.isin(required_skills_dict))\n",
    "filtered_candidates.show()\n",
    "candidates_with_skills = (filtered_candidates\n",
    "                          .groupBy(\"candidate_id\")\n",
    "                          .agg(countDistinct(\"skill\").alias(\"skill_count\"))\n",
    "                          .filter(col(\"skill_count\") == len(required_skills_dict))\n",
    "                          .orderBy(\"candidate_id\"))\n",
    "candidates_with_skills.show()\n",
    "\n",
    "list_df=df.groupby(\"candidate_id\").agg(sort_array(collect_list(\"skill\")).alias(\"skills\"))\n",
    "# when you use the array to compare there will be a datatype difference this below is the python list\n",
    "# not a java array list so we need to convert it into a datatype that spark can read using lit() method\n",
    "required_skills = sorted([\"Python\", \"Tableau\", \"PostgreSQL\"])\n",
    "required_skills_column = array([lit(skill) for skill in required_skills])\n",
    "list_df.show()\n",
    "print(required_skills_column)\n",
    "res = list_df.filter(list_df[\"skills\"] == required_skills_column)\n",
    "res.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"candidates\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    candidate_id\n",
    "FROM\n",
    "    candidates\n",
    "WHERE\n",
    "    skill IN ('Python', 'Tableau', 'PostgreSQL')\n",
    "GROUP BY\n",
    "    candidate_id\n",
    "HAVING\n",
    "    COUNT(skill) >= 3\n",
    "ORDER BY\n",
    "    candidate_id;\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3df89e",
   "metadata": {},
   "source": [
    "\n",
    "3. Assume you're given two tables containing data about Facebook Pages and their respective likes (as in \"Like a Facebook Page\").\n",
    "Write a query to return the IDs of the Facebook pages that have zero likes. The output should be sorted in ascending order based on the page IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b35eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|page_id|           page_name|\n",
      "+-------+--------------------+\n",
      "|  20001|       SQL Solutions|\n",
      "|  20045|     Brain Exercises|\n",
      "|  20701|Tips for Data Ana...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+-------+-------------------+\n",
      "|user_id|page_id|         liked_date|\n",
      "+-------+-------+-------------------+\n",
      "|    111|  20001|04/08/2022 00:00:00|\n",
      "|    121|  20045|03/12/2022 00:00:00|\n",
      "|    156|  20001|07/25/2022 00:00:00|\n",
      "+-------+-------+-------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|page_id|           page_name|\n",
      "+-------+--------------------+\n",
      "|  20701|Tips for Data Ana...|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|page_id|           page_name|\n",
      "+-------+--------------------+\n",
      "|  20701|Tips for Data Ana...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns1 = [\"page_id\",\t\"page_name\"]\n",
    "data1 = [\n",
    "    (\"20001\",\t\"SQL Solutions\"),\n",
    "    (\"20045\",\t\"Brain Exercises\"),\n",
    "    (\"20701\",\t\"Tips for Data Analysts\")\n",
    "]\n",
    "columns2=[\"user_id\",\t\"page_id\",\t\"liked_date\"]\n",
    "data2= [\n",
    "    (\"111\",\t\"20001\",\t\"04/08/2022 00:00:00\"),\n",
    "    (\"121\",\t\"20045\",\t\"03/12/2022 00:00:00\"),\n",
    "    (\"156\",\t\"20001\",\t\"07/25/2022 00:00:00\")\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        p.page_id,\n",
    "        p.page_name\n",
    "    from\n",
    "        df1 p\n",
    "    left anti join\n",
    "        df2 l\n",
    "    on\n",
    "        p.page_id=l.page_id\n",
    "    order by\n",
    "        page_id\n",
    "\"\"\").show()\n",
    "# df1.join(df2, \"page_id\", \"left_anti\" ).show()\n",
    "df1.join(df2, \"page_id\", \"anti\" ).orderBy(\"page_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f6bd6",
   "metadata": {},
   "source": [
    "4. Tesla is investigating production bottlenecks and they need your help to extract the relevant data.\n",
    "Write a query to determine which parts have begun the assembly process but are not yet finished.\n",
    "Assumptions:\n",
    "parts_assembly table contains all parts currently in production, each at varying stages of the assembly process.\n",
    "An unfinished part is one that lacks a finish_date.\n",
    "This question is straightforward, so let's approach it with simplicity in both thinking and solution.\n",
    "Effective April 11th 2023, the problem statement and assumptions were updated to enhance clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4621e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|   part|        finish_date|assembly_step|\n",
      "+-------+-------------------+-------------+\n",
      "|battery|01/22/2022 00:00:00|            1|\n",
      "|battery|02/22/2022 00:00:00|            2|\n",
      "|battery|03/22/2022 00:00:00|            3|\n",
      "| bumper|01/22/2022 00:00:00|            1|\n",
      "| bumper|02/22/2022 00:00:00|            2|\n",
      "| bumper|               NULL|            3|\n",
      "| bumper|               NULL|            4|\n",
      "+-------+-------------------+-------------+\n",
      "\n",
      "+------+-----------+-------------+\n",
      "|  part|finish_date|assembly_step|\n",
      "+------+-----------+-------------+\n",
      "|bumper|       NULL|            3|\n",
      "|bumper|       NULL|            4|\n",
      "+------+-----------+-------------+\n",
      "\n",
      "+------+-------------+\n",
      "|  part|assembly_step|\n",
      "+------+-------------+\n",
      "|bumper|            3|\n",
      "|bumper|            4|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns =[\"part\",\t\"finish_date\",\t\"assembly_step\"]\n",
    "data=[\n",
    "    (\"battery\",\t\"01/22/2022 00:00:00\",\"1\"),\n",
    "    (\"battery\",\t\"02/22/2022 00:00:00\",\"2\"),\n",
    "    (\"battery\",\t\"03/22/2022 00:00:00\",\"3\"),\n",
    "    (\"bumper\",   \"01/22/2022 00:00:00\",\"1\"),\n",
    "    (\"bumper\",   \"02/22/2022 00:00:00\",\"2\"),\n",
    "    (\"bumper\",None,\"3\"),\n",
    "    (\"bumper\",None,\"4\"),\n",
    " ]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.filter(df[\"finish_date\"].isNull()).show()\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        part,\n",
    "        assembly_step\n",
    "    from\n",
    "        df\n",
    "    where\n",
    "        finish_date is NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b626a",
   "metadata": {},
   "source": [
    "5. This is the same question as problem #3 in the SQL Chapter of Ace the Data Science Interview!\n",
    "Assume you're given the table on user viewership categorised by device type where the three types are laptop, tablet, and phone.\n",
    "Write a query that calculates the total viewership for laptops and mobile devices where mobile is defined as\n",
    "the sum of tablet and phone viewership. Output the total viewership for laptops as laptop_reviews and the total\n",
    "viewership for mobile devices as mobile_views.\n",
    "Effective 15 April 2023, the solution has been updated with a more concise and easy-to-understand approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee1f9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+\n",
      "|user_id|device_type|          view_time|\n",
      "+-------+-----------+-------------------+\n",
      "|    123|     tablet|01/02/2022 00:00:00|\n",
      "|    125|     laptop| 01/07/2022 00:00:0|\n",
      "|    128|     laptop| 02/09/2022 00:00:0|\n",
      "|    129|      phone| 02/09/2022 00:00:0|\n",
      "|    145|     tablet| 02/24/2022 00:00:0|\n",
      "+-------+-----------+-------------------+\n",
      "\n",
      "+------------+-----------+\n",
      "|laptop_views|mobile_view|\n",
      "+------------+-----------+\n",
      "|           2|          3|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+------------+\n",
      "|laptop_views|mobile_views|\n",
      "+------------+------------+\n",
      "|           2|           3|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"user_id\",\t\"device_type\",\"view_time\"]\n",
    "data = [\n",
    "    (\"123\",\t\"tablet\",\t\"01/02/2022 00:00:00\"),\n",
    "    (\"125\",\t\"laptop\",\t\"01/07/2022 00:00:0\"),\n",
    "    (\"128\",\t\"laptop\",\t\"02/09/2022 00:00:0\"),\n",
    "    (\"129\",\t\"phone\",\t\"02/09/2022 00:00:0\"),\n",
    "    (\"145\",\t\"tablet\",\t\"02/24/2022 00:00:0\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sum(case when device_type='laptop' then 1 else 0 end) as laptop_views,\n",
    "        sum(case when device_type in ('tablet', 'phone') then 1 else 0 end) as mobile_view\n",
    "    from df;\n",
    "\"\"\").show()\n",
    "\n",
    "df.agg(\n",
    "    sum(when(col(\"device_type\")=='laptop',1).otherwise(0)).alias(\"laptop_views\"),\n",
    "    sum(when(col(\"device_type\").isin(\"tablet\", \"phone\"),1).otherwise(0)).alias(\"mobile_views\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c67ec3",
   "metadata": {},
   "source": [
    "\n",
    "6.  Given a table of Facebook posts, for each user who posted at least twice in 2021, write a\n",
    "query to find the number of days between each user’s first post of the year and last post of the year\n",
    "in the year 2021. Output the user and number of the days between each user's first and last post.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebc1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+-------------------+\n",
      "|user_id|post_id|        post_content|          post_date|\n",
      "+-------+-------+--------------------+-------------------+\n",
      "| 151652| 599415|          Need a hug|2021-07-10 12:00:00|\n",
      "| 661093| 624356|Bed. Class 8-12. ...|2021-07-29 13:00:00|\n",
      "| 004239| 784254|  Happy 4th of July!|2021-07-04 11:00:00|\n",
      "| 661093| 442560|Just going to cry...|2021-07-08 14:00:00|\n",
      "| 151652| 111766|I'm so done with ...|2021-07-12 19:00:00|\n",
      "+-------+-------+--------------------+-------------------+\n",
      "\n",
      "+-------+--------------+\n",
      "|user_id|days_different|\n",
      "+-------+--------------+\n",
      "| 661093|            21|\n",
      "| 151652|             2|\n",
      "+-------+--------------+\n",
      "\n",
      "+-------+---------------+\n",
      "|user_id|days_difference|\n",
      "+-------+---------------+\n",
      "| 661093|             21|\n",
      "| 151652|              2|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"user_id\",\t\"post_id\",\t\"post_content\",\t\"post_date\"]\n",
    "data = [\n",
    "    (\"151652\",\t\"599415\",\t\"Need a hug\",\t\"07/10/2021 12:00:00\"),\n",
    "    (\"661093\",\t\"624356\",\t\"Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend\",\t\"07/29/2021 13:00:00\"),\n",
    "    (\"004239\",\t\"784254\",\t\"Happy 4th of July!\",\t\"07/04/2021 11:00:00\"),\n",
    "    (\"661093\",\t\"442560\",\t\"Just going to cry myself to sleep after watching Marley and Me.\",\t\"07/08/2021 14:00:00\"),\n",
    "    (\"151652\",\t\"111766\",\t\"I'm so done with covid - need travelling ASAP!\",\t\"07/12/2021 19:00:00\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"post_date\", to_timestamp(\"post_date\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        user_id,\n",
    "        date_diff(max(post_date),min(post_date)) as days_different\n",
    "    from\n",
    "        df\n",
    "    where\n",
    "        year(post_date)=2021\n",
    "    group by\n",
    "        user_id\n",
    "    having\n",
    "        count(post_id)>1\n",
    "    order by\n",
    "        days_different desc\n",
    "\"\"\").show()\n",
    "\n",
    "( df\n",
    "    .filter(year('post_date')==2021)\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(date_diff(max(\"post_date\"),min(\"post_date\")).alias(\"days_difference\"))\n",
    "    .filter(\"count(post_id)>1\")\n",
    "    .orderBy(col(\"days_difference\").desc())\n",
    "    .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ecfd0",
   "metadata": {},
   "source": [
    "\n",
    "7. Write a query to identify the top 2 Power Users who sent the highest number of messages on\n",
    " Microsoft Teams in August 2022. Display the IDs of these 2 users along with the total number of messages\n",
    " they sent. Output the results in descending order based on the count of the messages.\n",
    "Assumption:\n",
    "No two users have sent the same number of messages in August 2022.\n",
    "find the top 2 senders in the month of august 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2eb34a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+--------------------+-------------------+\n",
      "|message_id|sender_id|receiver_id|             content|          sent_date|\n",
      "+----------+---------+-----------+--------------------+-------------------+\n",
      "|       901|     3601|       4500|             You up?|2022-08-03 00:00:00|\n",
      "|       902|     4500|       3601|Only if you're bu...|2022-08-03 00:00:00|\n",
      "|       743|     3601|       8752|Let's take this o...|2022-06-14 00:00:00|\n",
      "|       922|     3601|       4500|     Get on the call|2022-08-10 00:00:00|\n",
      "+----------+---------+-----------+--------------------+-------------------+\n",
      "\n",
      "+---------+-----+\n",
      "|sender_id|count|\n",
      "+---------+-----+\n",
      "|     3601|    2|\n",
      "|     4500|    1|\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sender_id|count|\n",
      "+---------+-----+\n",
      "|     3601|    2|\n",
      "|     4500|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns=[\"message_id\",\t\"sender_id\",\t\"receiver_id\",\t\"content\",\t\"sent_date\"]\n",
    "data = [\n",
    "    (\"901\",\t\"3601\",\t\"4500\",\t\"You up?\",\t\"08/03/2022 00:00:00\"),\n",
    "    (\"902\",\t\"4500\",\t\"3601\",\t\"Only if you're buying\",\t\"08/03/2022 00:00:00\"),\n",
    "    (\"743\",\t\"3601\",\t\"8752\",\t\"Let's take this offline\",\t\"06/14/2022 00:00:00\"),\n",
    "    (\"922\",\t\"3601\",\t\"4500\",\t\"Get on the call\",\t\"08/10/2022 00:00:00\"),\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF(columns)\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"sent_date\", to_timestamp(\"sent_date\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# Perform filtering, grouping, counting, ordering, and limiting\n",
    "( df\n",
    "    .filter((year('sent_date')==2022)&(month('sent_date')==8))\n",
    "    .groupBy(\"sender_id\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .limit(2)\n",
    "    .show() \n",
    ")\n",
    "\n",
    "# df.filter(year('sent_date')==2022) & (month('sent_date')==8)).groupBy(\"sender_id\").count().orderBy(col(\"count\").desc()).limit(2).show()\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        sender_id,\n",
    "        count(*) as count\n",
    "    from\n",
    "        df\n",
    "    where\n",
    "       year(sent_date)=2022\n",
    "       and\n",
    "       month(sent_date)=8\n",
    "    group by\n",
    "        sender_id\n",
    "    order by\n",
    "        count desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e617e8",
   "metadata": {},
   "source": [
    "8. This is the same question as problem #8 in the SQL Chapter of Ace the Data Science Interview!\n",
    "Assume you're given a table containing job postings from various companies on the LinkedIn platform. Write a query to\n",
    "retrieve the count of companies that have posted duplicate job listings.\n",
    "Definition:\n",
    "Duplicate job listings are defined as two job listings within the same company that share identical titles and descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6d8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+--------------------+\n",
      "|job_id|company_id|           title|         description|\n",
      "+------+----------+----------------+--------------------+\n",
      "|   248|       827|Business Analyst|Business analyst ...|\n",
      "|   149|       845|Business Analyst|Business analyst ...|\n",
      "|   945|       345|    Data Analyst|Data analyst revi...|\n",
      "|   164|       345|    Data Analyst|Data analyst revi...|\n",
      "|   172|       244|   Data Engineer|Data engineer wor...|\n",
      "+------+----------+----------------+--------------------+\n",
      "\n",
      "+-------------------+\n",
      "|duplicate_companies|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "+-------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|duplicate_company_count|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "+-----------------------+\n",
      "\n",
      "+----------+\n",
      "|company_id|\n",
      "+----------+\n",
      "|       345|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"job_id\",\t\"company_id\",\t\"title\",\t\"description\"]\n",
    "\n",
    "data = [\n",
    "(\n",
    "    \"248\",\t\n",
    "    \"827\",\t\n",
    "    \"Business Analyst\",\t\n",
    "    \"\"\"Business analyst evaluates past and current business data with the primary \n",
    "    goal of improving decision-making processes within organizations.\"\"\"\n",
    "),\n",
    "(\n",
    "    \"149\",\t\n",
    "    \"845\",  \n",
    "    \"Business Analyst\",\t\n",
    "    \"\"\"Business analyst evaluates past and current business data with the primary \n",
    "    goal of improving decision-making processes within organizations.\"\"\"\n",
    "),\n",
    "(\n",
    "    \"945\",\t\n",
    "    \"345\",\t\n",
    "    \"Data Analyst\",     \n",
    "    \"\"\"Data analyst reviews data to identify key insights into a business's customers \n",
    "    and ways the data can be used to solve problems.\"\"\"\n",
    "),\n",
    "(\n",
    "    \"164\",\t\n",
    "    \"345\",\t\n",
    "    \"Data Analyst\",     \n",
    "    \"\"\"Data analyst reviews data to identify key insights into a business's customers \n",
    "    and ways the data can be used to solve problems.\"\"\"\n",
    "),\n",
    "(\n",
    "    \"172\",\t\n",
    "    \"244\",  \n",
    "    \"Data Engineer\",    \n",
    "    \"\"\"Data engineer works in a variety of settings to build systems that collect, \n",
    "    manage, and convert raw data into usable information for data scientists and \n",
    "    business analysts to interpret.\"\"\"\n",
    "),\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(columns)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH job_count_cte AS (\n",
    "          SELECT\n",
    "                company_id,\n",
    "                title,\n",
    "                description,\n",
    "                COUNT(job_id) AS job_count\n",
    "          FROM df\n",
    "          GROUP BY company_id, title, description\n",
    "    )\n",
    "    SELECT COUNT(DISTINCT company_id) AS duplicate_companies\n",
    "    FROM job_count_cte\n",
    "    WHERE job_count > 1;\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT company_id) AS duplicate_company_count\n",
    "FROM (\n",
    "    SELECT company_id, COUNT(*) AS cnt\n",
    "    FROM df\n",
    "    GROUP BY company_id, title, description\n",
    "    HAVING cnt > 1\n",
    ")\n",
    "\"\"\").show()\n",
    "\n",
    "( df\n",
    " .groupby(\"company_id\", \"title\", \"description\")\n",
    " .agg(count(\"*\").alias(\"count\"))\n",
    " .filter(\"count>1\")\n",
    " .select(\"company_id\")\n",
    " .distinct()\n",
    " .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d522193",
   "metadata": {},
   "source": [
    "9. This is the same question as problem #2 in the SQL Chapter of Ace the Data Science Interview!\n",
    "Assume you're given the tables containing completed trade orders and user details in a Robinhood trading system.\n",
    "Write a query to retrieve the top three cities that have the highest number of completed trade orders\n",
    "listed in descending order. Output the city name and the corresponding number of completed trade orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68fe30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+---------+-------------------+-----+\n",
      "|order_id|user_id|quantity|   status|               date|price|\n",
      "+--------+-------+--------+---------+-------------------+-----+\n",
      "|  100101|    111|      10|Cancelled|08/17/2022 12:00:00| 9.80|\n",
      "|  100102|    111|      10|Completed|08/17/2022 12:00:00|10.00|\n",
      "|  100259|    148|      35|Completed|08/25/2022 12:00:00| 5.10|\n",
      "|  100264|    148|      40|Completed|08/26/2022 12:00:00| 4.80|\n",
      "|  100305|    300|      15|Completed|09/05/2022 12:00:00|10.00|\n",
      "|  100400|    178|      32|Completed|09/17/2022 12:00:00|12.00|\n",
      "|  100565|    265|       2|Completed|09/27/2022 12:00:00| 8.70|\n",
      "+--------+-------+--------+---------+-------------------+-----+\n",
      "\n",
      "+-------+-------------+--------------------+-------------------+\n",
      "|user_id|         city|               email|        signup_date|\n",
      "+-------+-------------+--------------------+-------------------+\n",
      "|    111|San Francisco|    rrok10@gmail.com|08/03/2021 12:00:00|\n",
      "|    148|       Boston|sailor9820@gmail.com|08/20/2021 12:00:00|\n",
      "|    178|San Francisco|harrypotterfan182...|01/05/2022 12:00:00|\n",
      "|    265|       Denver|shadower_@hotmail...|02/26/2022 12:00:00|\n",
      "|    300|San Francisco|houstoncowboy1122...|06/30/2022 12:00:00|\n",
      "+-------+-------------+--------------------+-------------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|         city|count_orders|\n",
      "+-------------+------------+\n",
      "|San Francisco|           3|\n",
      "|       Boston|           2|\n",
      "|       Denver|           1|\n",
      "+-------------+------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|         city|count_orders|\n",
      "+-------------+------------+\n",
      "|San Francisco|           3|\n",
      "|       Boston|           2|\n",
      "|       Denver|           1|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns1=[\"order_id\",\t\"user_id\",\t\"quantity\",\t\"status\",\t\"date\",\t\"price\"]\n",
    "data1=[\n",
    "\t(\"100101\",\t\"111\",\t\"10\",\t\"Cancelled\",\t\"08/17/2022 12:00:00\",\t\"9.80\"),\n",
    "\t(\"100102\",\t\"111\",  \"10\",\"Completed\",\t\"08/17/2022 12:00:00\",\t\"10.00\"),\n",
    "\t(\"100259\",\t\"148\",  \"35\",\"Completed\",\"08/25/2022 12:00:00\",\t\"5.10\"),\n",
    "\t(\"100264\",\t\"148\",  \"40\",\"Completed\",\"08/26/2022 12:00:00\",\"4.80\"),\n",
    "\t(\"100305\",\t\"300\",  \"15\",\"Completed\",\"09/05/2022 12:00:00\",\"10.00\"),\n",
    "\t(\"100400\",\t\"178\",  \"32\",\"Completed\",\"09/17/2022 12:00:00\",\"12.00\"),\n",
    "\t(\"100565\",\t\"265\",  \"2\", \"Completed\",\"09/27/2022 12:00:00\",\"8.70\"),\n",
    "]\n",
    "\n",
    "\n",
    "columns2 = [\"user_id\",\t\"city\",\t\"email\",\t\"signup_date\"]\n",
    "data2 = [\n",
    "\t(\"111\",\t\"San Francisco\",\t\"rrok10@gmail.com\",\t\"08/03/2021 12:00:00\"),\n",
    "\t(\"148\",\t\"Boston\",\t\"sailor9820@gmail.com\",\t\"08/20/2021 12:00:00\"),\n",
    "\t(\"178\",\t\"San Francisco\",\t\"harrypotterfan182@gmail.com\",\t\"01/05/2022 12:00:00\"),\n",
    "\t(\"265\",\t\"Denver\",\t\"shadower_@hotmail.com\",\t\"02/26/2022 12:00:00\"),\n",
    "\t(\"300\",\t\"San Francisco\",\t\"houstoncowboy1122@hotmail.com\",\t\"06/30/2022 12:00:00\"),\n",
    "]\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)\n",
    "df1 = rdd1.toDF(columns1)\n",
    "df2 = rdd2.toDF(columns2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "spark.sql(\"\"\"\n",
    "        select\n",
    "        \tcity,\n",
    "\t\t\tcount(*) as count_orders\n",
    "        from\n",
    "            df1\n",
    "        join\n",
    "            df2\n",
    "        on\n",
    "            df1.user_id = df2.user_id\n",
    "        where\n",
    "        \tdf1.status='Completed'\n",
    "        group by\n",
    "        \tcity\n",
    "        order by\n",
    "        \tcount_orders desc\n",
    "        limit\n",
    "        \t3\n",
    "\"\"\").show()\n",
    "\n",
    "result_df = df1.join(df2, \"user_id\" ) \\\n",
    "\t.filter(df1[\"status\"] == 'Completed') \\\n",
    "\t.groupBy(\"city\") \\\n",
    "\t.agg(count(\"*\").alias(\"count_orders\")) \\\n",
    "\t.orderBy(col(\"count_orders\").desc()) \\\n",
    "\t.limit(3)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f4a7d",
   "metadata": {},
   "source": [
    "10.  Given the reviews table, write a query to retrieve the average star rating for each product, grouped by\n",
    "month. The output should display the month as a numerical value, product ID, and average star rating\n",
    "rounded to two decimal places. Sort the output first by month and then by product ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108b9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+----------+-----+\n",
      "|review_id|user_id|        submit_date|product_id|stars|\n",
      "+---------+-------+-------------------+----------+-----+\n",
      "|     6171|    123|2022-06-08 00:00:00|     50001|    4|\n",
      "|     7802|    265|2022-06-10 00:00:00|     69852|    4|\n",
      "|     5293|    362|2022-06-18 00:00:00|     50001|    3|\n",
      "|     6352|    192|2022-07-26 00:00:00|     69852|    3|\n",
      "|     4517|    981|2022-07-05 00:00:00|     69852|    2|\n",
      "+---------+-------+-------------------+----------+-----+\n",
      "\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- submit_date: timestamp (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      "\n",
      "+-----+----------+---+\n",
      "|month|product_id|avg|\n",
      "+-----+----------+---+\n",
      "|    6|     50001|3.5|\n",
      "|    6|     69852|4.0|\n",
      "|    7|     69852|2.5|\n",
      "+-----+----------+---+\n",
      "\n",
      "+-----+----------+---+\n",
      "|month|product_id|avg|\n",
      "+-----+----------+---+\n",
      "|    6|     50001|3.5|\n",
      "|    6|     69852|4.0|\n",
      "|    7|     69852|2.5|\n",
      "+-----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns= [ \"review_id\",\t\"user_id\",\t\"submit_date\",\t\"product_id\",\t\"stars\" ]\n",
    "data = [\n",
    "\t( \"6171\",\t\"123\",\t\"06/08/2022 00:00:00\",\t\"50001\",\t\"4\" ),\n",
    "\t( \"7802\",\t\"265\",\t\"06/10/2022 00:00:00\",\t\"69852\",\"4\" ),\n",
    "\t( \"5293\",\t\"362\",\t\"06/18/2022 00:00:00\",\t\"50001\",\"3\" ),\n",
    "\t( \"6352\",\t\"192\",\t\"07/26/2022 00:00:00\",\t\"69852\",\"3\"),\n",
    "\t( '4517',\t\"981\",\t\"07/05/2022 00:00:00\",\t\"69852\",\"2\" ),\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(columns)\n",
    "df = df.withColumn(\"submit_date\", to_timestamp(\"submit_date\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tmonth(submit_date) as month,\n",
    "\t\tproduct_id,\n",
    "\t\tavg(stars) as avg\n",
    "\tfrom\n",
    "\t\tdf\n",
    "\tgroup by\n",
    "\t\tmonth(submit_date),\n",
    "\t\tproduct_id\n",
    "\torder by\n",
    "\t\tmonth,\n",
    "\t\tproduct_id\n",
    "\"\"\").show()\n",
    "\n",
    "x = (df\n",
    "\t .withColumn(\"month\", month(\"submit_date\"))\n",
    "\t .groupBy(\"month\", \"product_id\")\n",
    "\t .agg(avg(\"stars\").alias(\"avg\"))\n",
    "\t.withColumn(\"avg\", round(\"avg\",2))\n",
    "\t .orderBy(\"month\", \"product_id\")\n",
    "\t )\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2343a",
   "metadata": {},
   "source": [
    "11. This is the same question as problem #1 in the SQL Chapter of Ace the Data Science Interview!\n",
    " Assume you have an events table on Facebook app analytics. Write a query to calculate the click-through rate (CTR)\n",
    " for the app in 2022 and round the results to 2 decimal places.\n",
    " Definition and\n",
    " note:Percentage of click-through rate (CTR) = 100.0 * Number of clicks / Number of impressions\n",
    " To avoid integer division, multiply the CTR by 100.0, not 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4405e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "+------+----------+-------------------+\n",
      "|app_id|event_type|          timestamp|\n",
      "+------+----------+-------------------+\n",
      "|   123|impression|2022-07-18 11:36:12|\n",
      "|   123|impression|2022-07-18 11:37:12|\n",
      "|   123|     click|2022-07-18 11:37:42|\n",
      "|   234|impression|2022-07-18 14:15:12|\n",
      "|   234|     click|2022-07-18 14:16:12|\n",
      "+------+----------+-------------------+\n",
      "\n",
      "+------+------------------+\n",
      "|app_id|click_through_rate|\n",
      "+------+------------------+\n",
      "|   234|             100.0|\n",
      "|   123|              50.0|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [ \"app_id\",\t\"event_type\",\t\"timestamp\" ]\n",
    "data = [\n",
    "\t( \"123\",\t\"impression\",\t\"07/18/2022 11:36:12\" ),\n",
    "\t( \"123\",\t\"impression\",\t\"07/18/2022 11:37:12\" ),\n",
    "\t( \"123\",\t\"click\",\t\"07/18/2022 11:37:42\" ),\n",
    "\t( \"234\",\t\"impression\",\t\"07/18/2022 14:15:12\" ),\n",
    "\t( \"234\",\t\"click\",\t\"07/18/2022 14:16:12\" ),\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(columns)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df.groupBy(\"app_id\").agg(\n",
    "    (\n",
    "        100.0\n",
    "        * sum(when(col(\"event_type\") == \"click\", 1).otherwise(0))\n",
    "        / sum(when(col(\"event_type\") == \"impression\", 1).otherwise(0))\n",
    "    ).alias(\"click_through_rate\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97916c98",
   "metadata": {},
   "source": [
    "\n",
    "12. Assume you're given tables with information about TikTok user sign-ups and confirmations through email and text. New users on TikTok sign up using their email addresses, and upon sign-up, each user receives a text message confirmation to activate their account.\n",
    "Write a query to display the user IDs of those who did not confirm their sign-up on the first day, but confirmed on the second day.\n",
    "Definition:\n",
    "action_date refers to the date when users activated their accounts and confirmed their sign-up through text messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29f63e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+\n",
      "|email_id|user_id|        signup_date|\n",
      "+--------+-------+-------------------+\n",
      "|     125|   7771|2022-06-14 00:00:00|\n",
      "|     433|   1052|2022-07-09 00:00:00|\n",
      "+--------+-------+-------------------+\n",
      "\n",
      "root\n",
      " |-- email_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      "\n",
      "+-------+--------+-------------+-------------------+\n",
      "|text_id|email_id|signup_action|        action_date|\n",
      "+-------+--------+-------------+-------------------+\n",
      "|   6878|     125|    Confirmed|2022-06-14 00:00:00|\n",
      "|   6997|     433|Not Confirmed|2022-07-09 00:00:00|\n",
      "|   7000|     433|    Confirmed|2022-07-10 00:00:00|\n",
      "+-------+--------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- text_id: string (nullable = true)\n",
      " |-- email_id: string (nullable = true)\n",
      " |-- signup_action: string (nullable = true)\n",
      " |-- action_date: timestamp (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|   1052|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|   1052|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns1 = [ \"email_id\",\t\"user_id\",\t\"signup_date\"]\n",
    "data = [\n",
    "\t( \"125\",\t\"7771\",\t\"06/14/2022 00:00:00\" ),\n",
    "\t( \"433\",\t\"1052\",\t\"07/09/2022 00:00:00\" ),\n",
    "]\n",
    "\n",
    "cols2=[ \"text_id\",\t\"email_id\",\t\"signup_action\",\t\"action_date\" ]\n",
    "data2 = [\n",
    "\t( \"6878\",\t\"125\",\t\"Confirmed\",\t\"06/14/2022 00:00:00\" ),\n",
    "\t( \"6997\",\t\"433\",\"Not Confirmed\",\"07/09/2022 00:00:00\" ),\n",
    "\t( \"7000\",\t\"433\",\"Confirmed\",\"07/10/2022 00:00:00\" ),\n",
    "]\n",
    "rdd1= sc.parallelize( data )\n",
    "rdd2= sc.parallelize( data2 )\n",
    "df1 = rdd1.toDF(columns1)\n",
    "df2 = rdd2.toDF(cols2)\n",
    "\n",
    "df1 =df1.withColumn(\"signup_date\", to_timestamp(\"signup_date\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df2=df2.withColumn(\"action_date\", to_timestamp(\"action_date\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df1.createOrReplaceTempView(\"emails\")\n",
    "df2.createOrReplaceTempView(\"texts\")\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "spark.sql(\"\"\"\n",
    "\tSELECT\n",
    "\t\tDISTINCT emails.user_id\n",
    "\tFROM\n",
    "\t\temails\n",
    "\tINNER JOIN\n",
    "\t\ttexts\n",
    "\tON\n",
    "\t\temails.email_id = texts.email_id\n",
    "\tWHERE\n",
    "\t\tDATE(texts.action_date) = DATE_ADD(DATE(emails.signup_date), 1)\n",
    "\t\tAND\n",
    "\t\ttexts.signup_action = 'Confirmed';\n",
    "\"\"\").show()\n",
    "\n",
    "res = (df1\n",
    "\t   .join(df2, \"email_id\", \"inner\")\n",
    "\t   .filter(\n",
    "\t\t\t(df2.action_date == date_add(df1.signup_date, 1))\n",
    "\t\t\t\t&\n",
    "\t\t\t(df2.signup_action=='Confirmed')\n",
    "\t\t)\n",
    "\t   .select(\"user_id\")\n",
    "\t   .distinct())\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dcb04",
   "metadata": {},
   "source": [
    "13. IBM is analyzing how their employees are utilizing the Db2 database by tracking the SQL queries\n",
    "executed by their employees. The objective is to generate data to populate a histogram that shows\n",
    "the number of unique queries run by employees during the third quarter of 2023 (July to September).\n",
    "Additionally, it should count the number of employees who did not run any queries during this period.\n",
    "Display the number of unique queries as histogram categories, along with the count of employees who\n",
    "executed that number of unique queries.\n",
    "need some iterations on the query need to figure out for the dsl and the spark sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b2f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+--------------+\n",
      "|employee_id|query_id|    query_starttime|execution_time|\n",
      "+-----------+--------+-------------------+--------------+\n",
      "|          3|  856987|2023-07-01 03:25:12|          2698|\n",
      "|          3|  286115|2023-07-01 04:34:38|          2705|\n",
      "|          3|   33683|2023-07-02 10:55:14|            91|\n",
      "|          1|  413477|2023-07-15 11:35:09|           470|\n",
      "|          1|  421983|2023-07-01 14:33:47|          3020|\n",
      "|          2|   17745|2023-07-01 14:33:47|          2093|\n",
      "|          2|  958745|2023-07-02 08:11:45|           512|\n",
      "|          2|  684293|2023-07-22 18:42:31|          1630|\n",
      "|          2|  385739|2023-07-25 14:25:17|           240|\n",
      "|          2|  123456|2023-07-26 16:12:18|           950|\n",
      "+-----------+--------+-------------------+--------------+\n",
      "\n",
      "+-----------+-----------------+------+\n",
      "|employee_id|        full_name|gender|\n",
      "+-----------+-----------------+------+\n",
      "|          1|    Judas Beardon|  Male|\n",
      "|          2|Lainey Franciotti|Female|\n",
      "|          3|   Ashbey Strahan|  Male|\n",
      "+-----------+-----------------+------+\n",
      "\n",
      "+--------------+--------------+\n",
      "|unique_queries|employee_count|\n",
      "+--------------+--------------+\n",
      "|             1|             1|\n",
      "|             2|             1|\n",
      "|             5|             1|\n",
      "+--------------+--------------+\n",
      "\n",
      "+--------------+--------------+\n",
      "|unique_queries|employee_count|\n",
      "+--------------+--------------+\n",
      "|             2|             1|\n",
      "|             3|             1|\n",
      "|             5|             1|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = [ \"employee_id\",\t\"query_id\",\t\"query_starttime\",\t\"execution_time\" ]\n",
    "data1 = [\n",
    "\t( \"3\",\t\"856987\",\t\"07/01/2023 03:25:12\",\t\"2698\" ),\n",
    "\t( \"3\",\t\"286115\",\t\"07/01/2023 04:34:38\",\t\"2705\" ),\n",
    "\t( \"3\",\t\"33683\",\"07/02/2023 10:55:14\",\t\"91\" ),\n",
    "\t( \"1\",\t\"413477\",\"07/15/2023 11:35:09\",\t\"470\" ),\n",
    "\t( \"1\", \"421983\", \"07/01/2023 14:33:47\", \"3020\" ),\n",
    "\t( \"2\",\t\"17745\",\"07/01/2023 14:33:47\",\t\"2093\" ),\n",
    "\t( \"2\", \"958745\", \"07/02/2023 08:11:45\", \"512\" ),\n",
    "\t( \"2\", \"684293\", \"07/22/2023 18:42:31\", \"1630\" ),\n",
    "\t( \"2\", \"385739\", \"07/25/2023 14:25:17\", \"240\" ),\n",
    "\t( \"2\", \"123456\", \"07/26/2023 16:12:18\", \"950\" ),\n",
    "]\n",
    "\n",
    "\n",
    "cols2 = [ \"employee_id\",\t\"full_name\",\t\"gender\" ]\n",
    "data2 = [\n",
    "\t( \"1\",\t\"Judas Beardon\",\t\"Male\" ),\n",
    "\t( \"2\",\t\"Lainey Franciotti\",\t\"Female\" ),\n",
    "\t( \"3\",\t\"Ashbey Strahan\",\t\"Male\" ),\n",
    "]\n",
    "rdd1 = sc.parallelize(data1)\n",
    "rdd2 = sc.parallelize(data2)\n",
    "df1 = rdd1.toDF(cols1)\n",
    "df2 = rdd2.toDF(cols2)\n",
    "df1 = df1.withColumn(\"query_starttime\", to_timestamp(\"query_starttime\",\"MM/dd/yyyy HH:mm:ss\"))\n",
    "df1.show()\n",
    "df2.show()\n",
    "df2.createOrReplaceTempView(\"employees\")\n",
    "df1.createOrReplaceTempView(\"queries\")\n",
    "spark.sql(\"\"\"\n",
    "WITH employee_queries AS (\n",
    "  SELECT\n",
    "    e.employee_id,\n",
    "    COALESCE(COUNT(DISTINCT q.query_id), 0) AS unique_queries\n",
    "  FROM employees AS e\n",
    "  LEFT JOIN queries AS q\n",
    "    ON e.employee_id = q.employee_id\n",
    "      AND q.query_starttime >= '2023-07-01T00:00:00Z'\n",
    "      AND q.query_starttime < '2023-10-01T00:00:00Z'\n",
    "  GROUP BY e.employee_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  unique_queries,\n",
    "  COUNT(employee_id) AS employee_count\n",
    "FROM employee_queries\n",
    "GROUP BY unique_queries\n",
    "ORDER BY unique_queries;\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "filtered_queries = df1.filter(\n",
    "\t(col(\"query_starttime\") >= \"2023-07-01\") &\n",
    "\t(col(\"query_starttime\") < \"2023-10-01\")\n",
    ")\n",
    "\n",
    "# Step 2: Perform the left join between employees and filtered queries\n",
    "joined_df = df2.join(\n",
    "\tfiltered_queries,\n",
    "\tdf2[\"employee_id\"] == filtered_queries[\"employee_id\"],\n",
    "\thow=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Group by employee_id and count distinct query_id\n",
    "employee_queries = joined_df.groupBy(df2[\"employee_id\"]).agg(\n",
    "\tcoalesce(countDistinct(\"query_id\"), lit(0)).alias(\"unique_queries\")\n",
    ")\n",
    "\n",
    "# Step 4: Group by unique_queries and count the number of employees in each category\n",
    "result = employee_queries.groupBy(\"unique_queries\").agg(\n",
    "\tcount(\"employee_id\").alias(\"employee_count\")\n",
    ").orderBy(\"unique_queries\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677222d",
   "metadata": {},
   "source": [
    "\n",
    "14. Your team at JPMorgan Chase is preparing to launch a new credit card, and to gain some insights,\n",
    "you're analyzing how many credit cards were issued each month.\n",
    "Write a query that outputs the name of each credit card and the difference in the number of issued\n",
    "cards between the month with the highest issuance cards and the lowest issuance. Arrange the results\n",
    "based on the largest disparity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f53d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+----------+\n",
      "|           card_name|issued_amount|issue_month|issue_year|\n",
      "+--------------------+-------------+-----------+----------+\n",
      "|  Chase Freedom Flex|        55000|          1|      2021|\n",
      "|  Chase Freedom Flex|        60000|          2|      2021|\n",
      "|  Chase Freedom Flex|        65000|          3|      2021|\n",
      "|  Chase Freedom Flex|        70000|          4|      2021|\n",
      "|Chase Sapphire Re...|       170000|          1|      2021|\n",
      "|Chase Sapphire Re...|       175000|          2|      2021|\n",
      "|Chase Sapphire Re...|       180000|          3|      2021|\n",
      "+--------------------+-------------+-----------+----------+\n",
      "\n",
      "+----------------------+----------+\n",
      "|card_name             |difference|\n",
      "+----------------------+----------+\n",
      "|Chase Freedom Flex    |15000     |\n",
      "|Chase Sapphire Reserve|10000     |\n",
      "+----------------------+----------+\n",
      "\n",
      "+----------------------+-----+\n",
      "|card_name             |diff |\n",
      "+----------------------+-----+\n",
      "|Chase Freedom Flex    |15000|\n",
      "|Chase Sapphire Reserve|10000|\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"card_name\",\t\"issued_amount\",\t\"issue_month\",\t\"issue_year\" ]\n",
    "data = [\n",
    "\t ( \"Chase Freedom Flex\",\t\"55000\",\t\"1\",\t\"2021\" ),\n",
    "\t ( \"Chase Freedom Flex\",\"60000\",\t\"2\",\t\"2021\" ),\n",
    "\t ( \"Chase Freedom Flex\",\"65000\",\t\"3\",\t\"2021\" ),\n",
    "\t ( \"Chase Freedom Flex\",\"70000\",\"4\",\t\"2021\" ),\n",
    "\t ( \"Chase Sapphire Reserve\",\t\"170000\",\t\"1\",\t\"2021\" ),\n",
    "\t ( \"Chase Sapphire Reserve\",\"175000\",\t\"2\",\t\"2021\" ),\n",
    "\t ( \"Chase Sapphire Reserve\",\"180000\",\t\"3\",\t\"2021\" ),\n",
    "]\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(cols)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"card_name\").orderBy(\"issue_month\")\n",
    "\n",
    "res = (df\n",
    "\t   .groupBy(\"card_name\")\n",
    "\t   .agg( (max(\"issued_amount\")-min(\"issued_amount\")).alias(\"difference\"))\n",
    "\t   .withColumn(\"difference\", floor(\"difference\"))\n",
    "\t)\n",
    "res.show(truncate=False)\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tcard_name,\n",
    "\t\tfloor(max(issued_amount)-min(issued_amount)) as diff\n",
    "\tfrom\n",
    "\t\tdf\n",
    "\tgroup by\n",
    "\t\tcard_name\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f78a55",
   "metadata": {},
   "source": [
    "\n",
    "15.  You('re trying to find the mean number of items per order on Alibaba, rounded to 1 decimal place using\n",
    "'tables which includes information on the count of items in each order (item_count table) and the '\n",
    "'corresponding number of orders for each item count (order_occurrences table).)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56e35d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|item_count|order_occurrences|\n",
      "+----------+-----------------+\n",
      "|         1|              500|\n",
      "|         2|             1000|\n",
      "|         3|              800|\n",
      "|         4|             1000|\n",
      "+----------+-----------------+\n",
      "\n",
      "+----+\n",
      "|mean|\n",
      "+----+\n",
      "| 2.7|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "|mean|\n",
      "+----+\n",
      "| 2.7|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [ \"item_count\",\t\"order_occurrences\" ]\n",
    "data = [\n",
    "\t( \"1\",\t\"500\" ),\n",
    "\t( \"2\",\t\"1000\") ,\n",
    "\t( \"3\",\t\"800\" ),\n",
    "\t( \"4\", \"1000\" ),\n",
    "]\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(cols)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tround(sum(item_count * order_occurrences) / sum(order_occurrences),2) as mean\n",
    "\tfrom df\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "(\n",
    "\tdf\n",
    "\t.agg(\n",
    "\t\tround(\n",
    "\t\t\tsum(df.item_count * df.order_occurrences) / sum(df.order_occurrences),2)\n",
    "\t\t\t\t.alias(\"mean\"))\n",
    " \t.show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025487d",
   "metadata": {},
   "source": [
    "\n",
    "16.  CVS Health is trying to better understand its pharmacy sales, and how well different products are\n",
    " selling. Each drug can only be produced by one manufacturer.\n",
    " Write a query to find the top 3 most profitable drugs sold, and how much profit they made. Assume that\n",
    " there are no ties in the profits. Display the result from the highest to the lowest total profit.\n",
    "Definition:\n",
    "cogs stands for Cost of Goods Sold which is the direct cost associated with producing the drug.\n",
    "Total Profit = Total Sales - Cost of Goods Sold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56efb5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+----------+------------+---------------+\n",
      "|product_id|units_sold|total_sales|      cogs|manufacturer|           drug|\n",
      "+----------+----------+-----------+----------+------------+---------------+\n",
      "|         9|     37410|  293452.54| 208876.01|   Eli Lilly|        Zyprexa|\n",
      "|        34|     94698|  600997.19| 521182.16| AstraZeneca|      Surmontil|\n",
      "|        61|     77023|  500101.61| 419174.97|      Biogen|Varicose Relief|\n",
      "|       136|    144814|    1084258|1006447.73|      Biogen|       Burkhart|\n",
      "+----------+----------+-----------+----------+------------+---------------+\n",
      "\n",
      "+---------------+-----------------+\n",
      "|           drug|          profits|\n",
      "+---------------+-----------------+\n",
      "|      Surmontil|79815.02999999997|\n",
      "|Varicose Relief|80926.64000000001|\n",
      "|        Zyprexa|84576.52999999997|\n",
      "+---------------+-----------------+\n",
      "\n",
      "+---------------+-----------------+\n",
      "|           drug|          profits|\n",
      "+---------------+-----------------+\n",
      "|        Zyprexa|84576.52999999997|\n",
      "|Varicose Relief|80926.64000000001|\n",
      "|      Surmontil|79815.02999999997|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [ \"product_id\",\t\"units_sold\",\t\"total_sales\",\t\"cogs\",\t\"manufacturer\",\t\"drug\" ]\n",
    "data = [\n",
    "\t( \"9\",\t\"37410\",\t\"293452.54\",\t\"208876.01\",\t\"Eli Lilly\",\t\"Zyprexa\" ),\n",
    "\t( \"34\",\t\"94698\",\t\"600997.19\",\t\"521182.16\",\t\"AstraZeneca\",\t\"Surmontil\" ),\n",
    "\t( \"61\",\t\"77023\",\t\"500101.61\",\t\"419174.97\",\t\"Biogen\",\t\"Varicose Relief\" ),\n",
    "\t( \"136\",\t\"144814\",\t\"1084258\",\t\"1006447.73\",\"Biogen\",\t\"Burkhart\" ),\n",
    "]\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(cols)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tdrug,\n",
    "\t\ttotal_sales - cogs as profits\n",
    "\tfrom\n",
    "\t\tdf\n",
    "\torder by\n",
    "\t\ttotal_sales desc\n",
    "\tlimit\n",
    "\t\t3\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "df.withColumn(\"profits\", df.total_sales.cast(\"double\") - df.cogs.cast(\"double\")) \\\n",
    "\t.select(\"drug\", \"profits\") \\\n",
    "\t.orderBy(col(\"profits\").desc()) \\\n",
    "\t.limit(3) \\\n",
    "\t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b427347",
   "metadata": {},
   "source": [
    "17. CVS Health is analyzing its pharmacy sales data, and how well different products are selling\n",
    "in the market. Each drug is exclusively manufactured by a single manufacturer.\n",
    "Write a query to identify the manufacturers associated with the drugs that resulted in losses for CVS\n",
    "Health and calculate the total amount of losses incurred.\n",
    "Output the manufacturer's name, the number of drugs associated with losses, and the total losses in\n",
    "absolute value. Display the results sorted in descending order with the highest losses displayed at the top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f32767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+----------+------------+--------------------+\n",
      "|product_id|units_sold|total_sales|      cogs|manufacturer|                drug|\n",
      "+----------+----------+-----------+----------+------------+--------------------+\n",
      "|       156|     89514|  3130097.0|3427421.73|      Biogen|           Acyclovir|\n",
      "|        25|    222331|  2753546.0|2974975.36|      AbbVie|Lamivudine and Zi...|\n",
      "|        50|     90484| 2521023.73| 2742445.9|   Eli Lilly|Dermasorb TA Comp...|\n",
      "|        98|    110746|  813188.82| 140422.87|      Biogen|          Medi-Chord|\n",
      "+----------+----------+-----------+----------+------------+--------------------+\n",
      "\n",
      "+------------+----------+------------------+\n",
      "|manufacturer|drug_count|        total_loss|\n",
      "+------------+----------+------------------+\n",
      "|      AbbVie|         1|221429.35999999987|\n",
      "|      Biogen|         1|         297324.73|\n",
      "|   Eli Lilly|         1|221422.16999999993|\n",
      "+------------+----------+------------------+\n",
      "\n",
      "+------------+------------------+-----+\n",
      "|manufacturer|        total_loss|count|\n",
      "+------------+------------------+-----+\n",
      "|      AbbVie|221429.35999999987|    1|\n",
      "|      Biogen|         297324.73|    1|\n",
      "|   Eli Lilly|221422.16999999993|    1|\n",
      "+------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"product_id\", \"units_sold\", \"total_sales\", \"cogs\", \"manufacturer\", \"drug\"]\n",
    "data = [\n",
    "\t(156, 89514, 3130097.00, 3427421.73, \"Biogen\", \"Acyclovir\"),\n",
    "\t(25, 222331, 2753546.00, 2974975.36, \"AbbVie\", \"Lamivudine and Zidovudine\"),\n",
    "\t(50, 90484, 2521023.73, 2742445.90, \"Eli Lilly\", \"Dermasorb TA Complete Kit\"),\n",
    "\t(98, 110746, 813188.82, 140422.87, \"Biogen\", \"Medi-Chord\")\n",
    "]\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(cols)\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tmanufacturer,\n",
    "\t\tcount(drug) as drug_count,\n",
    "\t\tsum(cogs-total_sales) as total_loss\n",
    "\tfrom\n",
    "\t\tdf\n",
    "\twhere\n",
    "\t\tcogs > total_sales\n",
    "\tgroup by\n",
    "\t\tmanufacturer\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "( df\n",
    "  \t.filter(\"cogs>total_sales\")\n",
    "  \t.groupby(\"manufacturer\")\n",
    "  \t.agg(\n",
    "\t\tsum(df.cogs-df.total_sales).alias(\"total_loss\"),\n",
    "\t\tcount(\"drug\").alias(\"count\")\n",
    "\t)\n",
    "\t.show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edba6e7",
   "metadata": {},
   "source": [
    "\n",
    "18. CVS Health wants to gain a clearer understanding of its pharmacy sales and the performance\n",
    " of various products.\n",
    " Write a query to calculate the total drug sales for each manufacturer. Round the answer to the nearest\n",
    "\tmillion and report your results in descending order of total sales. In case of any duplicates,\n",
    "\tsort them alphabetically by the manufacturer name.\n",
    "\tSince this data will be displayed on a dashboard viewed by business stakeholders, please format\n",
    "\tyour results as follows: \"$36 million\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49f06772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+----------+------------+---------------+\n",
      "|product_id|units_sold|total_sales|      cogs|manufacturer|           drug|\n",
      "+----------+----------+-----------+----------+------------+---------------+\n",
      "|        94|    132362| 2041758.41| 1373721.7|      Biogen|      UP and UP|\n",
      "|         9|     37410|  293452.54| 208876.01|   Eli Lilly|        Zyprexa|\n",
      "|        50|     90484| 2521023.73| 2742445.9|   Eli Lilly|      Dermasorb|\n",
      "|        61|     77023|  500101.61| 419174.97|      Biogen|Varicose Relief|\n",
      "|       136|    144814|  1084258.0|1006447.73|      Biogen|       Burkhart|\n",
      "+----------+----------+-----------+----------+------------+---------------+\n",
      "\n",
      "+------------+----------+\n",
      "|manufacturer|sales_mill|\n",
      "+------------+----------+\n",
      "|      Biogen|$3 million|\n",
      "|   Eli Lilly|$2 million|\n",
      "+------------+----------+\n",
      "\n",
      "+------------+----------+\n",
      "|manufacturer| sales_mil|\n",
      "+------------+----------+\n",
      "|      Biogen|$3 million|\n",
      "|   Eli Lilly|$2 million|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"product_id\", \"units_sold\", \"total_sales\", \"cogs\", \"manufacturer\", \"drug\"]\n",
    "data = [\n",
    "\t(94, 132362, 2041758.41, 1373721.70, \"Biogen\", \"UP and UP\"),\n",
    "\t(9, 37410, 293452.54, 208876.01, \"Eli Lilly\", \"Zyprexa\"),\n",
    "\t(50, 90484, 2521023.73, 2742445.90, \"Eli Lilly\", \"Dermasorb\"),\n",
    "\t(61, 77023, 500101.61, 419174.97, \"Biogen\", \"Varicose Relief\"),\n",
    "\t(136, 144814, 1084258.00, 1006447.73, \"Biogen\", \"Burkhart\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tmanufacturer,\n",
    "\t\tconcat('$', floor(sum(total_sales)/1000000), ' million') as sales_mill\n",
    "\tfrom\n",
    "\t\tdf\n",
    "\tgroup by\n",
    "\t\tmanufacturer\n",
    "\torder by\n",
    "\t\tsum(total_sales) desc,\n",
    "\t\tmanufacturer\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "result_df = (\n",
    "\tdf.groupBy(\"manufacturer\")\n",
    "\t.agg(floor(sum(\"total_sales\") / 1000000).alias(\"sales_mil\"))  # Sum and convert to millions\n",
    "\t.select(\n",
    "\t\tcol(\"manufacturer\"),\n",
    "\t\tconcat(lit(\"$\"), col(\"sales_mil\"), lit(\" million\")).alias(\"sales_mil\")  # Format as required\n",
    "\t)\n",
    "\t.orderBy(col(\"sales_mil\").desc(), col(\"manufacturer\"))  # Order by sales_mil and manufacturer\n",
    ")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d648959",
   "metadata": {},
   "source": [
    "19. UnitedHealth Group (UHG) has a program called Advocate4Me, which allows policy holders\n",
    " (or, members) to call an advocate and receive support for their health care needs – whether that's\n",
    " claims and benefits support, drug coverage, pre- and post-authorisation, medical records, emergency\n",
    " assistance, or member portal services.\n",
    "Write a query to find how many UHG policy holders made three, or more calls, assuming each call is\n",
    "identified by the case_id column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd5177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------------------+------------------+\n",
      "|policy_holder_id|             case_id|       call_category|           call_date|call_duration_secs|\n",
      "+----------------+--------------------+--------------------+--------------------+------------------+\n",
      "|               1|f1d012f9-9d02-496...|emergency assistance|2023-04-13T19:16:53Z|               144|\n",
      "|               1|41ce8fb6-1ddd-4f5...|       authorisation|2023-05-25T09:09:30Z|               815|\n",
      "|               2|9b1af84b-eedb-4c2...|   claims assistance|2023-01-26T01:21:27Z|               992|\n",
      "|               2|8471a3d4-6fc7-4bb...|emergency assistance|2023-03-09T10:58:54Z|               128|\n",
      "|               2|38208fae-bad0-49b...|            benefits|2023-06-05T07:35:43Z|               619|\n",
      "+----------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n",
      "+-------------------+\n",
      "|policy_holder_count|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "+-------------------+\n",
      "\n",
      "the number of UHG policy holders with more than 3 or more calls is:  1\n"
     ]
    }
   ],
   "source": [
    "columns = [\"policy_holder_id\", \"case_id\", \"call_category\", \"call_date\", \"call_duration_secs\"]\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "\t(1, \"f1d012f9-9d02-4966-a968-bf6c5bc9a9fe\", \"emergency assistance\", \"2023-04-13T19:16:53Z\", 144),\n",
    "\t(1, \"41ce8fb6-1ddd-4f50-ac31-07bfcce6aaab\", \"authorisation\", \"2023-05-25T09:09:30Z\", 815),\n",
    "\t(2, \"9b1af84b-eedb-4c21-9730-6f099cc2cc5e\", \"claims assistance\", \"2023-01-26T01:21:27Z\", 992),\n",
    "\t(2, \"8471a3d4-6fc7-4bb2-9fc7-4583e3638a9e\", \"emergency assistance\", \"2023-03-09T10:58:54Z\", 128),\n",
    "\t(2, \"38208fae-bad0-49bf-99aa-7842ba2e37bc\", \"benefits\", \"2023-06-05T07:35:43Z\", 619)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show()\n",
    "spark.sql(\"\"\"\n",
    "\twith call_records as (\n",
    "\t\tselect\n",
    "\t\t\tpolicy_holder_id,\n",
    "\t\t\tcount(case_id) as call_count\n",
    "\t\tfrom\n",
    "\t\t\tdf\n",
    "\t\tgroup by\n",
    "\t\t\tpolicy_holder_id\n",
    "\t\thaving\n",
    "\t\t\tcount(case_id)>=3\n",
    "\t)\n",
    "\tselect\n",
    "\t\tcount(policy_holder_id) as policy_holder_count\n",
    "\tfrom\n",
    "\t\tcall_records\n",
    "\"\"\").show()\n",
    "\n",
    "res =df.groupby(\"policy_holder_id\").agg(count(\"case_id\").alias('call_count'))\n",
    "print(\"the number of UHG policy holders with more than 3 or more calls is: \",res.filter(res.call_count >= 3).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627454a0",
   "metadata": {},
   "source": [
    "\n",
    "20. As a data analyst on the Oracle Sales Operations team, you are given a list of salespeople’s deals,\n",
    " and the annual quota they need to hit.\n",
    " Write a query that outputs each employee id and whether they hit the quota or not ('yes' or 'no').\n",
    " Order the results by employee id in ascending order.\n",
    "Definitions:\n",
    "deal_size: Deals acquired by a salesperson in the year. Each salesperson may have more than 1 deal.\n",
    "quota: Total annual quota for each salesperson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d0c6cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|employee_id|made_quota|\n",
      "+-----------+----------+\n",
      "|        101|       yes|\n",
      "|        201|       yes|\n",
      "|        301|        no|\n",
      "+-----------+----------+\n",
      "\n",
      "+-----------+----------+\n",
      "|employee_id|made_quota|\n",
      "+-----------+----------+\n",
      "|        101|       yes|\n",
      "|        201|       yes|\n",
      "|        301|        no|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_quota = [\n",
    "\t(101, 500000),\n",
    "\t(201, 400000),\n",
    "\t(301, 600000)\n",
    "]\n",
    "columns_quota = [\"employee_id\", \"quota\"]\n",
    "quotas = spark.createDataFrame(data_quota, columns_quota)\n",
    "\n",
    "data_deal = [\n",
    "\t(101, 400000),\n",
    "\t(101, 300000),\n",
    "\t(201, 500000),\n",
    "\t(301, 500000)\n",
    "]\n",
    "columns_deal = [\"employee_id\", \"deal_size\"]\n",
    "\n",
    "deals= spark.createDataFrame(data_deal, columns_deal)\n",
    "quotas.createOrReplaceTempView(\"quotas\")\n",
    "deals.createOrReplaceTempView(\"deals\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tdeals.employee_id,\n",
    "\t\tcase\n",
    "\t\t\twhen sum(deals.deal_size) > q.quota then 'yes'\n",
    "\t\t\telse 'no'\n",
    "\t\tend as made_quota\n",
    "\tfrom\n",
    "\t\tdeals\n",
    "\tjoin\n",
    "\t\tquotas q\n",
    "\ton\n",
    "\t\tdeals.employee_id=q.employee_id\n",
    "\tgroup by\n",
    "\t\tdeals.employee_id, q.quota\n",
    "\torder by\n",
    "\t\tdeals.employee_id\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "(deals\n",
    " \t.join(quotas,  \"employee_id\",\"inner\")\n",
    " \t.groupBy(\"employee_id\", \"quota\")\n",
    " \t.agg(when(\n",
    "\t\t\tsum(deals.deal_size)> quotas.quota, 'yes')\n",
    "\t\t  \t\t\t\t.otherwise('no').alias(\"made_quota\")\n",
    "\t).select(\"employee_id\", \"made_quota\").show()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72da54f",
   "metadata": {},
   "source": [
    "\n",
    "21. Companies often perform salary analyses to ensure fair compensation practices. One useful\n",
    "analysis is to check if there are any employees earning more than their direct managers.\n",
    "As a HR Analyst, you're asked to identify all employees who earn more than their direct managers.\n",
    "The result should include the employee's ID and name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c96dd21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+-------------+----------+\n",
      "|employee_id|            name|salary|department_id|manager_id|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "|          1|   Emma Thompson|  3800|            1|         6|\n",
      "|          2|Daniel Rodriguez|  2230|            1|         7|\n",
      "|          3|    Olivia Smith|  7000|            1|         8|\n",
      "|          4|    Noah Johnson|  6800|            2|         9|\n",
      "|          5| Sophia Martinez|  1750|            1|        11|\n",
      "|          6|      Liam Brown| 13000|            3|      NULL|\n",
      "|          7|      Ava Garcia| 12500|            3|      NULL|\n",
      "|          8|   William Davis|  6800|            2|      NULL|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "\n",
      "+-----------+-------------+\n",
      "|employee_id|employee_name|\n",
      "+-----------+-------------+\n",
      "|          3| Olivia Smith|\n",
      "+-----------+-------------+\n",
      "\n",
      "+-----------+-------------+\n",
      "|employee_id|employee_name|\n",
      "+-----------+-------------+\n",
      "|          3| Olivia Smith|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(1, \"Emma Thompson\", 3800, 1, 6),\n",
    "\t(2, \"Daniel Rodriguez\", 2230, 1, 7),\n",
    "\t(3, \"Olivia Smith\", 7000, 1, 8),\n",
    "\t(4, \"Noah Johnson\", 6800, 2, 9),\n",
    "\t(5, \"Sophia Martinez\", 1750, 1, 11),\n",
    "\t(6, \"Liam Brown\", 13000, 3, None),\n",
    "\t(7, \"Ava Garcia\", 12500, 3, None),\n",
    "\t(8, \"William Davis\", 6800, 2, None)\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department_id\", \"manager_id\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  emp.employee_id,\n",
    "  emp.name as employee_name\n",
    "from\n",
    "\tdf mgr\n",
    "INNER JOIN\n",
    "\tdf emp\n",
    "ON\n",
    "\tmgr.employee_id=emp.manager_id\n",
    "where\n",
    "\temp.salary>mgr.salary\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "df.alias(\"mgr\") \\\n",
    "\t.join(df.alias(\"emp\"), col(\"mgr.employee_id\") == col(\"emp.manager_id\")) \\\n",
    "\t.filter(col(\"emp.salary\") > col(\"mgr.salary\")) \\\n",
    "\t.select(col(\"emp.employee_id\"), col(\"emp.name\").alias(\"employee_name\")) \\\n",
    "\t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e150fb0",
   "metadata": {},
   "source": [
    "\n",
    "22.Assume you are given the table below on Uber transactions made by users. Write a query to obtain the\n",
    " third transaction of every user. Output the user id, spend and transaction date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8e425b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+\n",
      "|user_id|spend|   transaction_date|\n",
      "+-------+-----+-------------------+\n",
      "|    111|100.5|01/08/2022 12:00:00|\n",
      "|    111| 55.0|01/10/2022 12:00:00|\n",
      "|    121| 36.0|01/18/2022 12:00:00|\n",
      "|    145|24.99|01/26/2022 12:00:00|\n",
      "|    111| 89.6|02/05/2022 12:00:00|\n",
      "+-------+-----+-------------------+\n",
      "\n",
      "+-------+-----+-------------------+\n",
      "|user_id|spend|   transaction_date|\n",
      "+-------+-----+-------------------+\n",
      "|    111| 89.6|2022-02-05 12:00:00|\n",
      "+-------+-----+-------------------+\n",
      "\n",
      "+-------+-----+-------------------+\n",
      "|user_id|spend|   transaction_date|\n",
      "+-------+-----+-------------------+\n",
      "|    111| 89.6|2022-02-05 12:00:00|\n",
      "+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(111, 100.50, \"01/08/2022 12:00:00\"),\n",
    "\t(111, 55.00, \"01/10/2022 12:00:00\"),\n",
    "\t(121, 36.00, \"01/18/2022 12:00:00\"),\n",
    "\t(145, 24.99, \"01/26/2022 12:00:00\"),\n",
    "\t(111, 89.60, \"02/05/2022 12:00:00\")\n",
    "]\n",
    "cols = [\"user_id\", \"spend\", \"transaction_date\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show()\n",
    "df = df.withColumn(\"transaction_date\", to_timestamp(df.transaction_date , \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\twith cte as(\n",
    "\t\tselect\n",
    "\t\t\tuser_id,\n",
    "\t\t\tspend,\n",
    "\t\t\ttransaction_date,\n",
    "\t\t\trow_number()over(partition by user_id order by transaction_date) as row_num\n",
    "\t\tfrom df\n",
    "\t)\n",
    "\tselect\n",
    "\t\tuser_id,\n",
    "\t\tspend,\n",
    "\t\ttransaction_date\n",
    "\tfrom cte\n",
    "\twhere row_num=3\n",
    "\"\"\").show()\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n",
    "(\n",
    "\tdf\n",
    "\t.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\t.filter(\"row_num=3\")\n",
    "\t.select(\"user_id\", \"spend\", \"transaction_date\")\n",
    "\t.show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fa096",
   "metadata": {},
   "source": [
    "\n",
    "23.   Imagine you're an HR analyst at a tech company tasked with analyzing employee salaries.\n",
    "Your manager is keen on understanding the pay distribution and asks you to determine the second highest\n",
    "salary among all employees.\n",
    "It's possible that multiple employees may share the same second highest salary. In case of duplicate,\n",
    "display the salary only once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7e20ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+-------------+----------+\n",
      "|employee_id|            name|salary|department_id|manager_id|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "|          1|   Emma Thompson|  3800|            1|         6|\n",
      "|          2|Daniel Rodriguez|  2230|            1|         7|\n",
      "|          3|    Olivia Smith|  2000|            1|         8|\n",
      "|          4|          Aakash|  2000|            2|         8|\n",
      "|          4|          Nikhil|  4000|            2|         8|\n",
      "|          4|    Olivia Smith|  3000|            2|         8|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "\n",
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3800|\n",
      "+------+\n",
      "\n",
      "+-----------+------+-------------+----+\n",
      "|employee_id|salary|department_id|rank|\n",
      "+-----------+------+-------------+----+\n",
      "|          1|  3800|            1|   1|\n",
      "|          2|  2230|            1|   2|\n",
      "|          3|  2000|            1|   3|\n",
      "|          4|  4000|            2|   1|\n",
      "|          4|  3000|            2|   2|\n",
      "|          4|  2000|            2|   3|\n",
      "+-----------+------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(1, \"Emma Thompson\", 3800, 1, 6),\n",
    "\t(2, \"Daniel Rodriguez\", 2230, 1, 7),\n",
    "\t(3, \"Olivia Smith\", 2000, 1, 8),\n",
    "\t(4, \"Aakash\", 2000, 2, 8),\n",
    "\t(4, \"Nikhil\", 4000, 2, 8),\n",
    "\t(4, \"Olivia Smith\", 3000, 2, 8),\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department_id\", \"manager_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show()\n",
    "window_spec = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.withColumn(\"rank\", dense_rank().over(window_spec)).filter(\"rank=2\").select(\"salary\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\twith rank as(\n",
    "\t\tselect\n",
    "\t\t\temployee_id, salary, department_id,\n",
    "\t\t\tdense_rank() over (partition by department_id order by salary desc) as rank\n",
    "\t\tfrom df\n",
    "\t)\n",
    "\tselect\n",
    "\t*\t\n",
    "\tfrom rank\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4eb67",
   "metadata": {},
   "source": [
    "\n",
    "24. Assume you're given tables with information on Snapchat users, including their ages and time spent\n",
    "sending and opening snaps.\n",
    "Write a query to obtain a breakdown of the time spent sending vs. opening snaps as a percentage of\n",
    "total time spent on these activities grouped by age group. Round the percentage to 2 decimal places in\n",
    "the output.\n",
    "Notes:\n",
    "Calculate the following percentages:\n",
    "time spent sending / (Time spent sending + Time spent opening)\n",
    "Time spent opening / (Time spent sending + Time spent opening)\n",
    "To avoid integer division in percentages, multiply by 100.0 and not 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd4b93ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+----------+-------------------+\n",
      "|activity_id|user_id|activity_type|time_spent|      activity_date|\n",
      "+-----------+-------+-------------+----------+-------------------+\n",
      "|       7274|    123|         open|       4.5|06/22/2022 12:00:00|\n",
      "|       2425|    123|         send|       3.5|06/22/2022 12:00:00|\n",
      "|       1413|    456|         send|      5.67|06/23/2022 12:00:00|\n",
      "|       1414|    789|         chat|      11.0|06/25/2022 12:00:00|\n",
      "|       2536|    456|         open|       3.0|06/25/2022 12:00:00|\n",
      "+-----------+-------+-------------+----------+-------------------+\n",
      "\n",
      "+-------+----------+\n",
      "|user_id|age_bucket|\n",
      "+-------+----------+\n",
      "|    123|     31-35|\n",
      "|    456|     26-30|\n",
      "|    789|     21-25|\n",
      "+-------+----------+\n",
      "\n",
      "+----------+----------+---------+---------+\n",
      "|age_bucket|total_time|time_open|time_send|\n",
      "+----------+----------+---------+---------+\n",
      "|     21-25|      11.0|     11.0|      0.0|\n",
      "|     31-35|       8.0|      4.5|      3.5|\n",
      "|     26-30|      8.67|      3.0|     5.67|\n",
      "+----------+----------+---------+---------+\n",
      "\n",
      "+----------+----------+---------+---------+------------------+-----------------+\n",
      "|age_bucket|total_time|time_open|time_send|         open_perc|        send_perc|\n",
      "+----------+----------+---------+---------+------------------+-----------------+\n",
      "|     21-25|      11.0|     11.0|      0.0|             100.0|              0.0|\n",
      "|     31-35|       8.0|      4.5|      3.5|             56.25|            43.75|\n",
      "|     26-30|      8.67|      3.0|     5.67|34.602076124567475|65.39792387543253|\n",
      "+----------+----------+---------+---------+------------------+-----------------+\n",
      "\n",
      "+----------+------------------+-----------------+\n",
      "|age_bucket|         open_perc|        spend_per|\n",
      "+----------+------------------+-----------------+\n",
      "|     21-25|             100.0|              0.0|\n",
      "|     31-35|             56.25|            43.75|\n",
      "|     26-30|34.602076124567475|65.39792387543253|\n",
      "+----------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [ \"activity_id\", \"user_id\", \"activity_type\", \"time_spent\", \"activity_date\"]\n",
    "cols2 = [\"user_id\", \"age_bucket\"]\n",
    "activity_data = [\n",
    "\t(7274, 123, \"open\", 4.50, \"06/22/2022 12:00:00\"),\n",
    "\t(2425, 123, \"send\", 3.50, \"06/22/2022 12:00:00\"),\n",
    "\t(1413, 456, \"send\", 5.67, \"06/23/2022 12:00:00\"),\n",
    "\t(1414, 789, \"chat\", 11.00, \"06/25/2022 12:00:00\"),\n",
    "\t(2536, 456, \"open\", 3.00, \"06/25/2022 12:00:00\")\n",
    "]\n",
    "\n",
    "age_data = [\n",
    "\t(123, \"31-35\"),\n",
    "\t(456, \"26-30\"),\n",
    "\t(789, \"21-25\")\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(activity_data, cols)\n",
    "df2 = spark.createDataFrame(age_data, cols2)\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "\n",
    "df = df2.join(df1, \"user_id\",\"left\")\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "res = (\n",
    "df\n",
    ".groupBy(\"age_bucket\")\n",
    ".agg(\n",
    "    sum(\n",
    "        when(col(\"activity_type\").isin(\"chat\",\"open\", \"send\"), col(\"time_spent\"))\n",
    "        .otherwise(0)).alias(\"total_time\"),\n",
    "    sum(\n",
    "        when(col(\"activity_type\").isin(\"chat\",\"open\"), col(\"time_spent\"))\n",
    "        .otherwise(0)).alias(\"time_open\"),\n",
    "    sum(\n",
    "        when(col(\"activity_type\").isin(\"send\"), col(\"time_spent\"))\n",
    "        .otherwise(0)).alias(\"time_send\")\n",
    "))\n",
    "res.show()\n",
    "dff = (res\n",
    "\t  .withColumn(\"open_perc\", (res.time_open/res.total_time) * 100.0)\n",
    "\t  .withColumn(\"send_perc\", (res.time_send/res.total_time) * 100.0)\n",
    ")\n",
    "dff.show()\n",
    "spark.sql(\"\"\"\n",
    "\twith time as (\n",
    "\t\tselect\n",
    "\t\t\tage_bucket,\n",
    "\t\t\tsum(time_spent) as total_time,\n",
    "\t\t\tsum( case\n",
    "\t\t\t\t\twhen activity_type in ( 'chat', 'open' ) then time_spent\n",
    "\t\t\t\t\telse 0\n",
    "\t\t\t\t\tend\n",
    "\t\t\t) as time_open,\n",
    "\t\t\tsum( case\n",
    "\t\t\t\t\twhen activity_type in ( 'send' ) then time_spent\n",
    "\t\t\t\t\telse 0\n",
    "\t\t\t\t\tend\n",
    "\t\t\t) as time_send\n",
    "\t\tfrom\n",
    "\t\t\tdf\n",
    "\t\tgroup by\n",
    "\t\t\tage_bucket\n",
    "\t)\n",
    "\tselect\n",
    "\t\tage_bucket,\n",
    "\t\t(time_open/total_time) * 100.0 as open_perc,\n",
    "\t\t(time_send/total_time) * 100.0 as spend_per\n",
    "\tfrom time\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37ec31",
   "metadata": {},
   "source": [
    "\n",
    "25. Given a table of tweet data over a specified time period, calculate the 3-day rolling average\n",
    " of tweets for each user. Output the user ID, tweet date, and rolling averages rounded to 2 decimal places.\n",
    " Notes: A rolling average, also known as a moving average or running mean is a time-series technique that\n",
    " examines trends in data over a specified period of time.\n",
    " In this case, we want to determine how the tweet count for each user changes over a 3-day period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65463bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+\n",
      "|user_id|tweet_date         |tweet_count|\n",
      "+-------+-------------------+-----------+\n",
      "|111    |06/01/2022 00:00:00|2          |\n",
      "|111    |06/02/2022 00:00:00|1          |\n",
      "|111    |06/03/2022 00:00:00|3          |\n",
      "|111    |06/04/2022 00:00:00|4          |\n",
      "|111    |06/05/2022 00:00:00|5          |\n",
      "|114    |06/03/2022 00:00:00|3          |\n",
      "|114    |06/04/2022 00:00:00|4          |\n",
      "|114    |06/05/2022 00:00:00|5          |\n",
      "+-------+-------------------+-----------+\n",
      "\n",
      "+-------+-------------------+------------------+\n",
      "|user_id|         tweet_date|rolling_avg_3_days|\n",
      "+-------+-------------------+------------------+\n",
      "|    111|06/01/2022 00:00:00|               2.0|\n",
      "|    111|06/02/2022 00:00:00|               1.5|\n",
      "|    111|06/03/2022 00:00:00|               2.0|\n",
      "|    111|06/04/2022 00:00:00|              2.67|\n",
      "|    111|06/05/2022 00:00:00|               4.0|\n",
      "|    114|06/03/2022 00:00:00|               3.0|\n",
      "|    114|06/04/2022 00:00:00|               3.5|\n",
      "|    114|06/05/2022 00:00:00|               4.0|\n",
      "+-------+-------------------+------------------+\n",
      "\n",
      "with spark DSL\n",
      "+-------+-------------------+-----------+------------------+\n",
      "|user_id|         tweet_date|tweet_count|rolling_avg_3_days|\n",
      "+-------+-------------------+-----------+------------------+\n",
      "|    111|06/01/2022 00:00:00|          2|               2.0|\n",
      "|    111|06/02/2022 00:00:00|          1|               1.5|\n",
      "|    111|06/03/2022 00:00:00|          3|               2.0|\n",
      "|    111|06/04/2022 00:00:00|          4|2.6666666666666665|\n",
      "|    111|06/05/2022 00:00:00|          5|               4.0|\n",
      "|    114|06/03/2022 00:00:00|          3|               3.0|\n",
      "|    114|06/04/2022 00:00:00|          4|               3.5|\n",
      "|    114|06/05/2022 00:00:00|          5|               4.0|\n",
      "+-------+-------------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(111, \"06/01/2022 00:00:00\", 2),\n",
    "\t(111, \"06/02/2022 00:00:00\", 1),\n",
    "\t(111, \"06/03/2022 00:00:00\", 3),\n",
    "\t(111, \"06/04/2022 00:00:00\", 4),\n",
    "\t(111, \"06/05/2022 00:00:00\", 5),\n",
    "\t(114, \"06/03/2022 00:00:00\", 3),\n",
    "\t(114, \"06/04/2022 00:00:00\", 4),\n",
    "\t(114, \"06/05/2022 00:00:00\", 5)\n",
    "]\n",
    "cols = [\"user_id\", \"tweet_date\", \"tweet_count\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect\n",
    "\t\tuser_id,\n",
    "\t\ttweet_date,\n",
    "\t\tround(avg(tweet_count) over(partition by user_id order by tweet_date rows between 2 preceding and current row),2) as rolling_avg_3_days\n",
    "\tfrom\n",
    "\t\tdf\n",
    "\"\"\").show()\n",
    "print(\"with spark DSL\")\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"tweet_date\").rowsBetween(-2,0)\n",
    "df.withColumn(\"rolling_avg_3_days\", avg(\"tweet_count\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f0972",
   "metadata": {},
   "source": [
    "\n",
    "26. Assume you're given a table containing data on Amazon customers and their spending on products in\n",
    "different category, write a query to identify the top two highest-grossing products within each\n",
    "category in the year 2022. The output should include the category, product, and total spend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee87c220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------+------+-------------------+\n",
      "|   category|         product|user_id| spend|   transaction_date|\n",
      "+-----------+----------------+-------+------+-------------------+\n",
      "|  appliance|    refrigerator|    165| 246.0|2021-12-26 12:00:00|\n",
      "|  appliance|    refrigerator|    123|299.99|2022-03-02 12:00:00|\n",
      "|  appliance| washing machine|    123| 219.8|2022-03-02 12:00:00|\n",
      "|electronics|          vacuum|    178| 152.0|2022-04-05 12:00:00|\n",
      "|electronics|wireless headset|    156| 249.9|2022-07-08 12:00:00|\n",
      "|electronics|          vacuum|    145| 189.0|2022-07-15 12:00:00|\n",
      "+-----------+----------------+-------+------+-------------------+\n",
      "\n",
      "+-----------+----------------+-----------+\n",
      "|   category|         product|total_spend|\n",
      "+-----------+----------------+-----------+\n",
      "|  appliance|    refrigerator|     299.99|\n",
      "|  appliance| washing machine|      219.8|\n",
      "|electronics|          vacuum|      341.0|\n",
      "|electronics|wireless headset|      249.9|\n",
      "+-----------+----------------+-----------+\n",
      "\n",
      "+-----------+----------------+-----------+\n",
      "|   category|         product|total_spend|\n",
      "+-----------+----------------+-----------+\n",
      "|  appliance|    refrigerator|     299.99|\n",
      "|  appliance| washing machine|      219.8|\n",
      "|electronics|          vacuum|      341.0|\n",
      "|electronics|wireless headset|      249.9|\n",
      "+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(\"appliance\", \"refrigerator\", 165, 246.00, \"2021-12-26 12:00:00\"),\n",
    "\t(\"appliance\", \"refrigerator\", 123, 299.99, \"2022-03-02 12:00:00\"),\n",
    "\t(\"appliance\", \"washing machine\", 123, 219.80, \"2022-03-02 12:00:00\"),\n",
    "\t(\"electronics\", \"vacuum\", 178, 152.00, \"2022-04-05 12:00:00\"),\n",
    "\t(\"electronics\", \"wireless headset\", 156, 249.90, \"2022-07-08 12:00:00\"),\n",
    "\t(\"electronics\", \"vacuum\", 145, 189.00, \"2022-07-15 12:00:00\")\n",
    "]\n",
    "cols = [\"category\",\t\"product\",\t\"user_id\",\t\"spend\",\t\"transaction_date\"]\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df=df.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\twith rank as(\n",
    "\t\tselect\n",
    "\t\t\tcategory,\n",
    "\t\t\tproduct,\n",
    "\t\t\tsum(spend) as total_spend,\n",
    "\t\t\trank() over(partition by category order by sum(spend) desc) as rank\n",
    "\t\tfrom\n",
    "\t\t\tdf\n",
    "\t\twhere\n",
    "\t\t\tyear(transaction_date)=2022\n",
    "\t\tgroup by\n",
    "\t\t\tcategory,\n",
    "\t\t\tproduct\n",
    "\t)\n",
    "\tselect\n",
    "\t\tcategory,\n",
    "\t\tproduct,\n",
    "\t\ttotal_spend\n",
    "\tfrom rank\n",
    "\twhere rank <=2\n",
    "\torder by category, rank\n",
    "\"\"\").show()\n",
    "\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(col(\"total_spend\").desc())\n",
    "\n",
    "m1 = df.withColumn(\"year\", year(\"transaction_date\"))\n",
    "m2 = (m1\n",
    "\t  .filter(col(\"year\")==2022)\n",
    "\t  .groupby(\"category\", \"product\")\n",
    "\t  .agg(sum(\"spend\").alias(\"total_spend\"))\n",
    "\t  .withColumn(\"rank\", rank().over(window_spec))\n",
    "\t  .filter(col(\"rank\")<=2)\n",
    "\t  .orderBy(\"category\", \"rank\")\n",
    "\t  .select(\"category\", \"product\", \"total_spend\")\n",
    "\t)\n",
    "m2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45128059",
   "metadata": {},
   "source": [
    "\n",
    "27.  As part of an ongoing analysis of salary distribution within the company, your manager has\n",
    " requested a report identifying high earners in each department. A 'high earner' within a department\n",
    " is defined as an employee with a salary ranking among the top three salaries within that department.\n",
    " You're tasked with identifying these high earners across all departments. Write a query to display the\n",
    " employee's name along with their department name and salary. In case of duplicates, sort the results of\n",
    " department name in ascending order, then by salary in descending order. If multiple employees have\n",
    " the same salary, then order them alphabetically.\n",
    "Note: Ensure to utilize the appropriate ranking window function to handle duplicate salaries effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1648bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+-------------+----------+\n",
      "|employee_id|   employee_name|salary|department_id|manager_id|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "|          1|   Emma Thompson|  3800|            1|         6|\n",
      "|          2|Daniel Rodriguez|  2230|            1|         7|\n",
      "|          3|    Olivia Smith|  2000|            1|         8|\n",
      "|          4|    Noah Johnson|  6800|            2|         9|\n",
      "|          5| Sophia Martinez|  1750|            1|        11|\n",
      "|          6|      Liam Brown| 13000|            3|      NULL|\n",
      "|          7|      Ava Garcia| 12500|            3|      NULL|\n",
      "|          8|   William Davis|  6800|            2|      NULL|\n",
      "|          9| Isabella Wilson| 11000|            3|      NULL|\n",
      "|         10|  James Anderson|  4000|            1|        11|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "\n",
      "+-------------+---------------+\n",
      "|department_id|department_name|\n",
      "+-------------+---------------+\n",
      "|            1| Data Analytics|\n",
      "|            2|   Data Science|\n",
      "+-------------+---------------+\n",
      "\n",
      "+---------------+----------------+------+\n",
      "|department_name|   employee_name|salary|\n",
      "+---------------+----------------+------+\n",
      "| Data Analytics|  James Anderson|  4000|\n",
      "| Data Analytics|   Emma Thompson|  3800|\n",
      "| Data Analytics|Daniel Rodriguez|  2230|\n",
      "|   Data Science|    Noah Johnson|  6800|\n",
      "|   Data Science|   William Davis|  6800|\n",
      "+---------------+----------------+------+\n",
      "\n",
      "+---------------+----------------+------+\n",
      "|department_name|   employee_name|salary|\n",
      "+---------------+----------------+------+\n",
      "| Data Analytics|  James Anderson|  4000|\n",
      "| Data Analytics|   Emma Thompson|  3800|\n",
      "| Data Analytics|Daniel Rodriguez|  2230|\n",
      "|   Data Science|    Noah Johnson|  6800|\n",
      "|   Data Science|   William Davis|  6800|\n",
      "+---------------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_data = [\n",
    "\t(1, \"Emma Thompson\", 3800, 1, 6),\n",
    "\t(2, \"Daniel Rodriguez\", 2230, 1, 7),\n",
    "\t(3, \"Olivia Smith\", 2000, 1, 8),\n",
    "\t(4, \"Noah Johnson\", 6800, 2, 9),\n",
    "\t(5, \"Sophia Martinez\", 1750, 1, 11),\n",
    "\t(6, \"Liam Brown\", 13000, 3, None),\n",
    "\t(7, \"Ava Garcia\", 12500, 3, None),\n",
    "\t(8, \"William Davis\", 6800, 2, None),\n",
    "\t(9, \"Isabella Wilson\", 11000, 3, None),\n",
    "\t(10, \"James Anderson\", 4000, 1, 11)\n",
    "]\n",
    "cols = [\"employee_id\",\"employee_name\",\"salary\", \"department_id\", \"manager_id\"]\n",
    "cols2 = [\"department_id\", \"department_name\"]\n",
    "\n",
    "# Department data\n",
    "department_data = [\n",
    "\t(1, \"Data Analytics\"),\n",
    "\t(2, \"Data Science\")\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(employee_data, cols)\n",
    "df2 = spark.createDataFrame(department_data, cols2)\n",
    "df1.show()\n",
    "df2.show()\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "\n",
    "\n",
    "res = df1.join(df2, \"department_id\", 'left')\n",
    "window_spec = Window.partitionBy(\"department_id\").orderBy(col(\"salary\").desc(), col(\"employee_name\"))\n",
    "res = (\n",
    "\tres\n",
    "\t.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "\t.filter(\"department_id!=3\")\n",
    "\t.select(\"department_name\", \"employee_name\", \"salary\")\n",
    "\t.filter(\"rank<=3\")\n",
    "\t)\n",
    "res.show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\twith rank as(\n",
    "\t\tselect\n",
    "\t\t\tdf1.department_id,\n",
    "\t\t\tdf1.salary,\n",
    "\t\t\tdepartment_name,\n",
    "\t\t\temployee_name,\n",
    "\t\t\tdense_rank()over(partition by df1.department_id order by df1.salary desc, df1.employee_name) as rank\n",
    "\t\tfrom\n",
    "\t\t\tdf1\n",
    "\t\tleft join\n",
    "\t\t\tdf2\n",
    "\t\ton\n",
    "\t\t\tdf1.department_id=df2.department_id\n",
    "\t\twhere\n",
    "\t\t\tdf1.department_id!=3\n",
    "\t)\n",
    "\tselect\n",
    "\t\tdepartment_name,\n",
    "\t\temployee_name,\n",
    "\t\tsalary\n",
    "\tfrom\n",
    "\t\trank\n",
    "\twhere\n",
    "\t\trank<=3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac58c72",
   "metadata": {},
   "source": [
    "\n",
    "28.  Assume there are three Spotify tables: artists, songs, and global_song_rank, which contain\n",
    "information about the artists, songs, and music charts, respectively.\n",
    "Write a query to find the top 5 artists whose songs appear most frequently in the Top 10 of the\n",
    "global_song_rank table. Display the top 5 artist names in ascending order, along with their song\n",
    "appearance ranking.\n",
    "If two or more artists have the same number of song appearances, they should be assigned the same\n",
    "ranking, and the rank numbers should be continuous (i.e. 1, 2, 2, 3, 4, 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b96fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|artist_id|artist_name|        label_owner|\n",
      "+---------+-----------+-------------------+\n",
      "|      101| Ed Sheeran| Warner Music Group|\n",
      "|      120|      Drake| Warner Music Group|\n",
      "|      125|  Bad Bunny|Rimas Entertainment|\n",
      "+---------+-----------+-------------------+\n",
      "\n",
      "+-------+---------+-------------+\n",
      "|song_id|artist_id|         name|\n",
      "+-------+---------+-------------+\n",
      "|  55511|      101|      Perfect|\n",
      "|  45202|      101| Shape of You|\n",
      "|  22222|      120|    One Dance|\n",
      "|  19960|      120|Hotline Bling|\n",
      "+-------+---------+-------------+\n",
      "\n",
      "+---+-------+----+\n",
      "|day|song_id|rank|\n",
      "+---+-------+----+\n",
      "|  1|  45202|   5|\n",
      "|  3|  45202|   2|\n",
      "|  1|  19960|   3|\n",
      "|  9|  19960|  15|\n",
      "+---+-------+----+\n",
      "\n",
      "+-----------+----------+-----------+\n",
      "|artist_name|song_count|artist_rank|\n",
      "+-----------+----------+-----------+\n",
      "| Ed Sheeran|         2|          1|\n",
      "|      Drake|         1|          2|\n",
      "+-----------+----------+-----------+\n",
      "\n",
      "+-----------+-----------+\n",
      "|artist_name|artist_rank|\n",
      "+-----------+-----------+\n",
      "| Ed Sheeran|          1|\n",
      "|      Drake|          2|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+-----------+\n",
      "|artist_name|artist_rank|\n",
      "+-----------+-----------+\n",
      "| Ed Sheeran|          1|\n",
      "|      Drake|          2|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_data = [\n",
    "\t(101, \"Ed Sheeran\", \"Warner Music Group\"),\n",
    "\t(120, \"Drake\", \"Warner Music Group\"),\n",
    "\t(125, \"Bad Bunny\", \"Rimas Entertainment\")\n",
    "]\n",
    "\n",
    "# Songs data\n",
    "songs_data = [\n",
    "\t(55511, 101, \"Perfect\"),\n",
    "\t(45202, 101, \"Shape of You\"),\n",
    "\t(22222, 120, \"One Dance\"),\n",
    "\t(19960, 120, \"Hotline Bling\")\n",
    "]\n",
    "\n",
    "# Global song rank data\n",
    "global_song_rank_data = [\n",
    "\t(1, 45202, 5),\n",
    "\t(3, 45202, 2),\n",
    "\t(1, 19960, 3),\n",
    "\t(9, 19960, 15)\n",
    "]\n",
    "\n",
    "# Create DataFrames without specifying schema\n",
    "artists_df = spark.createDataFrame(artists_data, [\"artist_id\", \"artist_name\", \"label_owner\"])\n",
    "songs_df = spark.createDataFrame(songs_data, [\"song_id\", \"artist_id\", \"name\"])\n",
    "global_song_rank_df = spark.createDataFrame(global_song_rank_data, [\"day\", \"song_id\", \"rank\"])\n",
    "\n",
    "# Show DataFrames\n",
    "artists_df.show()\n",
    "songs_df.show()\n",
    "global_song_rank_df.show()\n",
    "artists_df.createOrReplaceTempView(\"artists\")\n",
    "songs_df.createOrReplaceTempView(\"songs\")\n",
    "global_song_rank_df.createOrReplaceTempView(\"ranking\")\n",
    "\n",
    "window_spec = Window.orderBy(col(\"song_count\").desc())\n",
    "\n",
    "# Perform the join and aggregation\n",
    "top_10_df = (artists_df\n",
    "\t\t\t .join(songs_df, \"artist_id\", \"inner\")\n",
    "\t\t\t .join(global_song_rank_df, \"song_id\", \"inner\")\n",
    "\t\t\t .filter(global_song_rank_df.rank <= 10)\n",
    "\t\t\t .groupBy(artists_df.artist_name)\n",
    "\t\t\t .agg(count(\"song_id\").alias(\"song_count\"))\n",
    "\t\t\t .withColumn(\"artist_rank\", dense_rank().over(window_spec))\n",
    "\t\t\t )\n",
    "top_10_df.show()\n",
    "final_result = top_10_df.filter(top_10_df.artist_rank <= 5).select(\"artist_name\", \"artist_rank\")\n",
    "final_result.show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tWITH top_10_cte as (\n",
    "\t  SELECT\n",
    "\t\tartists.artist_name,\n",
    "\t\tdense_rank()over(ORDER BY count(songs.song_id) desc) as artist_rank\n",
    "\t  from\n",
    "\t  \tartists\n",
    "\t  inner JOIN\n",
    "\t  \tsongs\n",
    "\t  ON\n",
    "\t  \tartists.artist_id = songs.artist_id\n",
    "\t  inner JOIN\n",
    "\t  \tranking\n",
    "\t  ON\n",
    "\t  \tsongs.song_id=ranking.song_id\n",
    "\t  WHERE\n",
    "\t  \tranking.rank <=10\n",
    "\t  GROUP BY\n",
    "\t  \tartists.artist_name\n",
    "\t)\n",
    "\n",
    "\tSELECT\n",
    "\t\tartist_name,\n",
    "\t\tartist_rank\n",
    "\tfrom\n",
    "\t\ttop_10_cte\n",
    "\tWHERE\n",
    "\t\tartist_rank<=5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ab9ae",
   "metadata": {},
   "source": [
    "29. New TikTok users sign up with their emails. They confirmed their signup by replying to the text\n",
    "confirmation to activate their accounts. Users may receive multiple text messages for account confirmation\n",
    "until they have confirmed their new account.\n",
    "A senior analyst is interested to know the activation rate of specified users in the emails table.\n",
    "Write a query to find the activation rate. Round the percentage to 2 decimal places.\n",
    "Definitions:\n",
    "emails table contain the information of user signup details.\n",
    "texts table contains the users' activation information.\n",
    "Assumptions:\n",
    "The analyst is interested in the activation rate of specific users in the emails table, which may\n",
    "not include all users that could potentially be found in the texts table.\n",
    "For example, user 123 in the emails table may not be in the texts table and vice versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ffcca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+\n",
      "|email_id|user_id|        signup_date|\n",
      "+--------+-------+-------------------+\n",
      "|     125|   7771|2022-06-14 00:00:00|\n",
      "|     236|   6950|2022-07-01 00:00:00|\n",
      "|     433|   1052|2022-07-09 00:00:00|\n",
      "+--------+-------+-------------------+\n",
      "\n",
      "+-------+--------+-------------+\n",
      "|text_id|email_id|signup_action|\n",
      "+-------+--------+-------------+\n",
      "|   6878|     125|    Confirmed|\n",
      "|   6920|     236|Not Confirmed|\n",
      "|   6994|     236|    Confirmed|\n",
      "+-------+--------+-------------+\n",
      "\n",
      "+---------------+\n",
      "|activation_rate|\n",
      "+---------------+\n",
      "|           0.67|\n",
      "+---------------+\n",
      "\n",
      "+---------------+\n",
      "|activation_rate|\n",
      "+---------------+\n",
      "|           0.67|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_data = [\n",
    "\t(125, 7771, \"2022-06-14 00:00:00\"),\n",
    "\t(236, 6950, \"2022-07-01 00:00:00\"),\n",
    "\t(433, 1052, \"2022-07-09 00:00:00\")\n",
    "]\n",
    "\n",
    "# Create emails DataFrame\n",
    "emails_df = spark.createDataFrame(emails_data, [\"email_id\", \"user_id\", \"signup_date\"])\n",
    "\n",
    "# Sample data for texts\n",
    "texts_data = [\n",
    "\t(6878, 125, \"Confirmed\"),\n",
    "\t(6920, 236, \"Not Confirmed\"),\n",
    "\t(6994, 236, \"Confirmed\")\n",
    "]\n",
    "\n",
    "# Create texts DataFrame\n",
    "texts_df = spark.createDataFrame(texts_data, [\"text_id\", \"email_id\", \"signup_action\"])\n",
    "\n",
    "# Show DataFrames\n",
    "emails_df.show()\n",
    "texts_df.show()\n",
    "\n",
    "emails_df.createOrReplaceTempView(\"emails\")\n",
    "texts_df.createOrReplaceTempView(\"texts\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "\tROUND(COUNT(texts.email_id)/COUNT(DISTINCT emails.email_id),2) AS activation_rate\n",
    "FROM emails\n",
    "LEFT JOIN texts\n",
    "ON emails.email_id = texts.email_id\n",
    "AND texts.signup_action = 'Confirmed';\n",
    "\"\"\").show()\n",
    "#\n",
    "\n",
    "\n",
    "result_df = (\n",
    "\temails_df\n",
    "\t.join(texts_df, emails_df.email_id == texts_df.email_id, \"left\")\n",
    "\t.agg(\n",
    "\t\tround(\n",
    "\t\t\tcount(when(texts_df.signup_action == 'Confirmed', texts_df.email_id)) / countDistinct(emails_df.email_id), 2\n",
    "\t\t).alias(\"activation_rate\")\n",
    "\t)\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294562eb",
   "metadata": {},
   "source": [
    "\n",
    "30. A Microsoft Azure Supercloud customer is defined as a customer who has purchased at\n",
    " least one product from every product category listed in the products table.\n",
    "Write a query that identifies the customer IDs of these Supercloud customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfee5fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|product_id|amount|\n",
      "+-----------+----------+------+\n",
      "|          1|         1|  1000|\n",
      "|          1|         6|  2000|\n",
      "|          1|         5|  1500|\n",
      "|          2|         2|  3000|\n",
      "|          2|         6|  2000|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+----------+----------------+--------------------+\n",
      "|product_id|product_category|        product_name|\n",
      "+----------+----------------+--------------------+\n",
      "|         1|       Analytics|    Azure Databricks|\n",
      "|         2|       Analytics|Azure Stream Anal...|\n",
      "|         4|      Containers|Azure Kubernetes ...|\n",
      "|         5|      Containers|Azure Service Fabric|\n",
      "|         6|         Compute|    Virtual Machines|\n",
      "|         7|         Compute|     Azure Functions|\n",
      "+----------+----------------+--------------------+\n",
      "\n",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_contracts_data = [\n",
    "\t(1, 1, 1000),\n",
    "\t(1, 6, 2000),\n",
    "\t(1, 5, 1500),\n",
    "\t(2, 2, 3000),\n",
    "\t(2, 6, 2000)\n",
    "]\n",
    "\n",
    "# Sample data for products\n",
    "products_data = [\n",
    "\t(1, \"Analytics\", \"Azure Databricks\"),\n",
    "\t(2, \"Analytics\", \"Azure Stream Analytics\"),\n",
    "\t(4, \"Containers\", \"Azure Kubernetes Service\"),\n",
    "\t(5, \"Containers\", \"Azure Service Fabric\"),\n",
    "\t(6, \"Compute\", \"Virtual Machines\"),\n",
    "\t(7, \"Compute\", \"Azure Functions\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "customer_contracts_df = spark.createDataFrame(customer_contracts_data, [\"customer_id\", \"product_id\", \"amount\"])\n",
    "products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_category\", \"product_name\"])\n",
    "\n",
    "# Show DataFrames\n",
    "customer_contracts_df.show()\n",
    "products_df.show()\n",
    "customer_contracts_df.createOrReplaceTempView(\"customer_contracts\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "spark.sql(\"\"\"\n",
    "\tWITH supercloud_cust AS (\n",
    "\t\tSELECT\n",
    "\t\t\tcustomers.customer_id,\n",
    "\t\t\tCOUNT(DISTINCT products.product_category) AS product_count\n",
    "\t\tFROM\n",
    "\t\t\tcustomer_contracts AS customers\n",
    "\t\tINNER JOIN\n",
    "\t\t\tproducts\n",
    "\t\tON\n",
    "\t\t\tcustomers.product_id = products.product_id\n",
    "\t\tGROUP BY\n",
    "\t\t\tcustomers.customer_id\n",
    "\t)\n",
    "\tSELECT\n",
    "\t\tcustomer_id\n",
    "\tFROM\n",
    "\t\tsupercloud_cust\n",
    "\tWHERE\n",
    "\t\tproduct_count = (\n",
    "\t\t\tSELECT\n",
    "\t\t\t\tCOUNT(DISTINCT product_category) FROM products\n",
    "\t);\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "supercloud_cust_df = ( customer_contracts_df\n",
    "  .join(products_df, \"product_id\", \"inner\")\n",
    "  .groupBy(\"customer_id\")\n",
    "  .agg(count_distinct(products_df.product_category).alias(\"product_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "total_product_category_count = (products_df\n",
    "\t\t\t\t\t\t\t\t\t.select(countDistinct(\"product_category\").alias(\"total_product_categories\"))\n",
    "\t\t\t\t\t\t\t\t\t.collect()[0][0])\n",
    "\n",
    "result_df = (\n",
    "\tsupercloud_cust_df\n",
    "\t.filter(supercloud_cust_df.product_count == total_product_category_count)\n",
    "\t.select(\"customer_id\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dfe95",
   "metadata": {},
   "source": [
    "31. This is the same question as problem #28 in the SQL Chapter of Ace the Data Science Interview!\n",
    "Assume you're given a table with measurement values obtained from a Google sensor over multiple days\n",
    "with measurements taken multiple times within each day.\n",
    "Write a query to calculate the sum of odd-numbered and even-numbered measurements separately for a\n",
    "particular day and display the results in two different columns. Refer to the Example Output below for the\n",
    "desired format.\n",
    "Definition:\n",
    "Within a day, measurements taken at 1st, 3rd, and 5th times are considered odd-numbered measurements,\n",
    "and measurements taken at 2nd, 4th, and 6th times are considered even-numbered measurements.\n",
    "\n",
    "info you would want to assign a row number to the date then you can just %2 the results as odd sum and even sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d8d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+----------------+\n",
      "|measurement_id|measurement_value|measurement_time|\n",
      "+--------------+-----------------+----------------+\n",
      "|        131233|          1109.51|      2022-07-10|\n",
      "|        135211|          1662.74|      2022-07-10|\n",
      "|        523542|          1246.24|      2022-07-10|\n",
      "|        143562|           1124.5|      2022-07-11|\n",
      "|        346462|          1234.14|      2022-07-11|\n",
      "+--------------+-----------------+----------------+\n",
      "\n",
      "+----------------+-------+--------+\n",
      "|measurement_time|odd_sum|even_sum|\n",
      "+----------------+-------+--------+\n",
      "|      2022-07-10|2355.75| 1662.74|\n",
      "|      2022-07-11| 1124.5| 1234.14|\n",
      "+----------------+-------+--------+\n",
      "\n",
      "+----------------+-------+--------+\n",
      "|measurement_time|odd_sum|even_sum|\n",
      "+----------------+-------+--------+\n",
      "|      2022-07-10|2355.75| 1662.74|\n",
      "|      2022-07-11| 1124.5| 1234.14|\n",
      "+----------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "measurement_data = [\n",
    "\t(131233, 1109.51, \"07/10/2022 09:00:00\"),\n",
    "\t(135211, 1662.74, \"07/10/2022 11:00:00\"),\n",
    "\t(523542, 1246.24, \"07/10/2022 13:15:00\"),\n",
    "\t(143562, 1124.50, \"07/11/2022 15:00:00\"),\n",
    "\t(346462, 1234.14, \"07/11/2022 16:45:00\")\n",
    "]\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"measurement_id\", \"measurement_value\", \"measurement_time\"]\n",
    "df = spark.createDataFrame(measurement_data, columns)\n",
    "df = df.withColumn(\"measurement_time\", to_date(df[\"measurement_time\"], \"MM/dd/yyyy HH:mm:ss\"))\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"measurement_time\").orderBy(col(\"measurement_time\").desc())\n",
    "row_df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "row_df.groupBy(\"measurement_time\").agg(\n",
    "\tsum(when(col(\"row_num\") % 2 != 0, col(\"measurement_value\")).otherwise(0)).alias(\"odd_sum\"),\n",
    "\tsum(when(col(\"row_num\") % 2 == 0, col(\"measurement_value\")).otherwise(0)).alias(\"even_sum\")\n",
    ").show()\n",
    "\n",
    "# info the key is to assign a row number and then % 2 that row number you will get the odd and even numbers sum\n",
    "spark.sql(\"\"\"\n",
    "\twith view as (\n",
    "\t\tselect\n",
    "\t\t\tmeasurement_time,\n",
    "\t\t\tmeasurement_value,\n",
    "\t\t\trow_number() over(partition by measurement_time order by measurement_time desc) as row_num\n",
    "\t\tfrom\n",
    "\t\t\tdf\n",
    "\t)\n",
    "\tselect\n",
    "\t\tmeasurement_time,\n",
    "\t\tsum(case when row_num%2!=0 then measurement_value else 0 end) as odd_sum,\n",
    "\t\tsum(case when row_num%2=0 then measurement_value else 0 end) as even_sum\n",
    "\tfrom\n",
    "\t\tview\n",
    "\tgroup by\n",
    "\t\tmeasurement_time\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29368b4c",
   "metadata": {},
   "source": [
    "\n",
    "32. Assume you're given a table on Walmart user transactions. Based on their most recent transaction date,\n",
    " write a query that retrieve the users along with the number of products they bought.\n",
    "Output the user's most recent transaction date, user ID, and the number of products, sorted in\n",
    "chronological order by the transaction date.\n",
    "Starting from November 10th, 2022, the official solution was updated, and the expected output of\n",
    "transaction date, number of users, and number of products was changed to the current expected output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d376035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+-------------------+\n",
      "|product_id|user_id|spend|transaction_date   |\n",
      "+----------+-------+-----+-------------------+\n",
      "|3673      |123    |68.9 |07/08/2022 12:00:00|\n",
      "|9623      |123    |274.1|07/08/2022 12:00:00|\n",
      "|1467      |115    |19.9 |07/08/2022 12:00:00|\n",
      "|2513      |159    |25.0 |07/08/2022 12:00:00|\n",
      "|1452      |159    |74.5 |07/10/2022 12:00:00|\n",
      "+----------+-------+-----+-------------------+\n",
      "\n",
      "+-------------------+-------+--------------+\n",
      "|   transaction_date|user_id|purchase_count|\n",
      "+-------------------+-------+--------------+\n",
      "|07/08/2022 12:00:00|    115|             1|\n",
      "|07/08/2022 12:00:00|    123|             2|\n",
      "|07/10/2022 12:00:00|    159|             1|\n",
      "+-------------------+-------+--------------+\n",
      "\n",
      "+-------------------+-------+-----+\n",
      "|   transaction_date|user_id|count|\n",
      "+-------------------+-------+-----+\n",
      "|07/08/2022 12:00:00|    115|    1|\n",
      "|07/08/2022 12:00:00|    123|    2|\n",
      "|07/10/2022 12:00:00|    159|    1|\n",
      "+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(3673, 123, 68.90, \"07/08/2022 12:00:00\"),\n",
    "\t(9623, 123, 274.10, \"07/08/2022 12:00:00\"),\n",
    "\t(1467, 115, 19.90, \"07/08/2022 12:00:00\"),\n",
    "\t(2513, 159, 25.00, \"07/08/2022 12:00:00\"),\n",
    "\t(1452, 159, 74.50, \"07/10/2022 12:00:00\")\n",
    "]\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"product_id\", \"user_id\", \"spend\", \"transaction_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "# we need to get the most latest transaction and show how many products bought on that transaction\n",
    "# we can rank all the transactions then filter out the rank=1 and then do the extra remaining logic\n",
    "spark.sql(\"\"\"\n",
    "\twith latest as (\n",
    "\t\tselect\n",
    "\t\t\ttransaction_date,\n",
    "\t\t\tuser_id,\n",
    "\t\t\tproduct_id,\n",
    "\t\t\trank() over(partition by user_id order by transaction_date desc) as trans_rank\n",
    "\t\tfrom\n",
    "\t\t\tdf\n",
    "\t)\n",
    "\tselect\n",
    "\t\ttransaction_date,\n",
    "\t\tuser_id,\n",
    "\t\tcount(product_id) as purchase_count\n",
    "\tfrom\n",
    "\t\tlatest\n",
    "\twhere\n",
    "\t\ttrans_rank=1\n",
    "\tgroup by\n",
    "\t\ttransaction_date,\n",
    "\t\tuser_id\n",
    "\torder by\n",
    "\t\ttransaction_date\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"transaction_date\").desc())\n",
    "df = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "res = df.filter(\"rank=1\").groupBy(\"transaction_date\", \"user_id\").agg(count(\"product_id\").alias(\"count\"))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54638444",
   "metadata": {},
   "source": [
    "\n",
    "33.   You're given a table containing the item count for each order on Alibaba, along with the\n",
    "frequency of orders that have the same item count. Write a query to retrieve the mode of the order\n",
    "occurrences. Additionally, if there are multiple item counts with the same mode, the results\n",
    "should be sorted in ascending order.\n",
    "Clarifications:\n",
    "item_count: Represents the number of items sold in each order.\n",
    "order_occurrences: Represents the frequency of orders with the corresponding number of items sold per order.\n",
    "For example, if there are 800 orders with 3 items sold in each order, the record would have an\n",
    "item_count of 3 and an order_occurrences of 800.\n",
    "\n",
    "\n",
    "you want to return the item_count for which the maximum order_occurrences are there if there are multiple\n",
    "order by item_count in asc order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "681550bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|item_count|order_occurrences|\n",
      "+----------+-----------------+\n",
      "|1         |500              |\n",
      "|2         |1000             |\n",
      "|3         |800              |\n",
      "+----------+-----------------+\n",
      "\n",
      "+----+\n",
      "|mode|\n",
      "+----+\n",
      "|   2|\n",
      "+----+\n",
      "\n",
      "1000\n",
      "+----+\n",
      "|mode|\n",
      "+----+\n",
      "|   2|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(1, 500),\n",
    "\t(2, 1000),\n",
    "\t(3, 800)\n",
    "]\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"item_count\", \"order_occurrences\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\tSELECT item_count AS mode\n",
    "FROM df\n",
    "WHERE order_occurrences = (\n",
    "  SELECT MAX(order_occurrences)\n",
    "  FROM df\n",
    ")\n",
    "ORDER BY item_count;\n",
    "\"\"\").show()\n",
    "max_order_occurrences = df.orderBy(col(\"order_occurrences\").desc()).first()\n",
    "max_value = max_order_occurrences[\"order_occurrences\"]\n",
    "print(max_value)\n",
    "res = df.filter(df.order_occurrences==max_value).select(col(\"item_count\").alias(\"mode\"))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df1892",
   "metadata": {},
   "source": [
    "\n",
    "34.  Your team at JPMorgan Chase is soon launching a new credit card. You are asked to estimate how many\n",
    "cards you'll issue in the first month.\n",
    "Before you can answer this question, you want to first get some perspective on how well new credit\n",
    "card launches typically do in their first month.\n",
    "Write a query that outputs the name of the credit card, and how many cards were issued in its launch\n",
    "month. The launch month is the earliest record in the monthly_cards_issued table for a given card.\n",
    "Order the results starting from the biggest issued amount.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdd32baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------------------+-------------+\n",
      "|issue_month|issue_year|card_name             |issued_amount|\n",
      "+-----------+----------+----------------------+-------------+\n",
      "|1          |2021      |Chase Sapphire Reserve|170000       |\n",
      "|2          |2021      |Chase Sapphire Reserve|175000       |\n",
      "|3          |2021      |Chase Sapphire Reserve|180000       |\n",
      "|3          |2021      |Chase Freedom Flex    |65000        |\n",
      "|4          |2021      |Chase Freedom Flex    |70000        |\n",
      "+-----------+----------+----------------------+-------------+\n",
      "\n",
      "+--------------------+-------------+\n",
      "|           card_name|issued_amount|\n",
      "+--------------------+-------------+\n",
      "|Chase Sapphire Re...|       170000|\n",
      "|  Chase Freedom Flex|        65000|\n",
      "+--------------------+-------------+\n",
      "\n",
      "+--------------------+-------------+\n",
      "|           card_name|issued_amount|\n",
      "+--------------------+-------------+\n",
      "|Chase Sapphire Re...|       170000|\n",
      "|  Chase Freedom Flex|        65000|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "\t(1, 2021, \"Chase Sapphire Reserve\", 170000),\n",
    "\t(2, 2021, \"Chase Sapphire Reserve\", 175000),\n",
    "\t(3, 2021, \"Chase Sapphire Reserve\", 180000),\n",
    "\t(3, 2021, \"Chase Freedom Flex\", 65000),\n",
    "\t(4, 2021, \"Chase Freedom Flex\", 70000)\n",
    "]\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"issue_month\", \"issue_year\", \"card_name\", \"issued_amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\twith ranked as (\n",
    "\t\tselect\n",
    "\t\t\tcard_name,\n",
    "\t\t\trow_number()over(partition by card_name order by issue_year, issue_month) as row_number,\n",
    "\t\t\tissued_amount\n",
    "\t\tfrom df\n",
    "\t)\n",
    "\tselect\n",
    "\t\tcard_name,\n",
    "\t\tissued_amount\n",
    "\tfrom\n",
    "\t\tranked\n",
    "\twhere\n",
    "\t\trow_number=1\n",
    "\torder by\n",
    "\t\tissued_amount desc\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"card_name\").orderBy(\"issue_year\", \"issue_month\")\n",
    "res = (df\n",
    "\t   .withColumn(\"rank\", row_number().over(window_spec))\n",
    "\t   .filter(\"rank=1\")\n",
    "\t   .select(\"card_name\",\"issued_amount\")\n",
    "\t   .orderBy(col(\"issued_amount\").desc())\n",
    "\t   )\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572129e",
   "metadata": {},
   "source": [
    "\n",
    "35. A phone call is considered an international call when the person calling is in a different\n",
    " country than the person receiving the call.\n",
    "What percentage of phone calls are international? Round the result to 1 decimal.\n",
    "Assumption:\n",
    "The caller_id in phone_info table refers to both the caller and receiver.\n",
    "phone_calls Table:\n",
    "\n",
    "Data for phone_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac935617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|caller_id|receiver_id|call_time          |\n",
      "+---------+-----------+-------------------+\n",
      "|1        |2          |2022-07-04 10:13:49|\n",
      "|1        |5          |2022-08-21 23:54:56|\n",
      "|5        |1          |2022-05-13 17:24:06|\n",
      "|5        |6          |2022-03-18 12:11:49|\n",
      "|5        |6          |2022-03-18 12:11:56|\n",
      "|5        |6          |2022-03-18 12:11:59|\n",
      "+---------+-----------+-------------------+\n",
      "\n",
      "+---------+----------+--------+---------------+\n",
      "|caller_id|country_id|network |phone_number   |\n",
      "+---------+----------+--------+---------------+\n",
      "|1        |US        |Verizon |+1-212-897-1964|\n",
      "|2        |US        |Verizon |+1-703-346-9529|\n",
      "|3        |US        |Verizon |+1-650-828-4774|\n",
      "|4        |US        |Verizon |+1-415-224-6663|\n",
      "|5        |IN        |Vodafone|+91 7503-907302|\n",
      "|6        |IN        |Vodafone|+91 2287-664895|\n",
      "+---------+----------+--------+---------------+\n",
      "\n",
      "+----------------------+\n",
      "|international_call_pct|\n",
      "+----------------------+\n",
      "|                  33.3|\n",
      "+----------------------+\n",
      "\n",
      "+---------+-----------+-----------------+-------------------+\n",
      "|caller_id|receiver_id|caller_country_id|receiver_country_id|\n",
      "+---------+-----------+-----------------+-------------------+\n",
      "|        5|          6|               IN|                 IN|\n",
      "|        5|          6|               IN|                 IN|\n",
      "|        5|          6|               IN|                 IN|\n",
      "|        1|          5|               US|                 IN|\n",
      "|        5|          1|               IN|                 US|\n",
      "|        1|          2|               US|                 US|\n",
      "+---------+-----------+-----------------+-------------------+\n",
      "\n",
      "+----------------------+\n",
      "|international_call_pct|\n",
      "+----------------------+\n",
      "|                  33.3|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_calls_data = [\n",
    "\t(1, 2, \"2022-07-04 10:13:49\"),\n",
    "\t(1, 5, \"2022-08-21 23:54:56\"),\n",
    "\t(5, 1, \"2022-05-13 17:24:06\"),\n",
    "\t(5, 6, \"2022-03-18 12:11:49\"),\n",
    "\t(5, 6, \"2022-03-18 12:11:56\"),\n",
    "\t(5, 6, \"2022-03-18 12:11:59\")\n",
    "]\n",
    "phone_calls_columns = [\"caller_id\", \"receiver_id\", \"call_time\"]\n",
    "phone_calls_df = spark.createDataFrame(phone_calls_data, phone_calls_columns)\n",
    "phone_calls_df.show(truncate=False)\n",
    "\n",
    "phone_info_data = [\n",
    "\t(1, \"US\", \"Verizon\", \"+1-212-897-1964\"),\n",
    "\t(2, \"US\", \"Verizon\", \"+1-703-346-9529\"),\n",
    "\t(3, \"US\", \"Verizon\", \"+1-650-828-4774\"),\n",
    "\t(4, \"US\", \"Verizon\", \"+1-415-224-6663\"),\n",
    "\t(5, \"IN\", \"Vodafone\", \"+91 7503-907302\"),\n",
    "\t(6, \"IN\", \"Vodafone\", \"+91 2287-664895\")\n",
    "]\n",
    "phone_info_columns = [\"caller_id\", \"country_id\", \"network\", \"phone_number\"]\n",
    "phone_info_df = spark.createDataFrame(phone_info_data, phone_info_columns)\n",
    "phone_info_df.show(truncate=False)\n",
    "\n",
    "phone_calls_df.createOrReplaceTempView(\"phone_calls\")\n",
    "phone_info_df.createOrReplaceTempView(\"phone_info\")\n",
    "res = spark.sql(\"\"\"\n",
    "\tWITH international_calls AS (\n",
    "\t\tSELECT\n",
    "\t\t  caller.caller_id,\n",
    "\t\t  caller.country_id,\n",
    "\t\t  receiver.caller_id,\n",
    "\t\t  receiver.country_id\n",
    "\t\tFROM\n",
    "\t\t\tphone_calls AS calls\n",
    "\t\tLEFT JOIN\n",
    "\t\t\tphone_info AS caller\n",
    "\t\tON\n",
    "\t\t\tcalls.caller_id = caller.caller_id\n",
    "\t\tLEFT JOIN\n",
    "\t\t\tphone_info AS receiver\n",
    "\t\tON\n",
    "\t\t\tcalls.receiver_id = receiver.caller_id\n",
    "\t\tWHERE\n",
    "\t\t\tcaller.country_id <> receiver.country_id\n",
    "\t)\n",
    "\n",
    "\tSELECT\n",
    "\t  ROUND(100.0 * COUNT(*)/ (SELECT COUNT(*) FROM phone_calls),1) AS international_call_pct\n",
    "\tFROM international_calls;\n",
    "\"\"\")\n",
    "res.show()\n",
    "\n",
    "result_df = (\n",
    "\tphone_calls_df\n",
    "\t.join(\n",
    "\t\t\tphone_info_df.alias(\"caller_info\"),\n",
    "\t\t\tphone_calls_df.caller_id == col(\"caller_info.caller_id\"),\n",
    "\t\t\t\"inner\"\n",
    "\t)\n",
    "\t.join(\n",
    "\t\tphone_info_df.alias(\"receiver_info\"),\n",
    "\t\tphone_calls_df.receiver_id == col(\"receiver_info.caller_id\"),\n",
    "\t\t\"inner\"\n",
    "\t)\n",
    "\t.select(\n",
    "\t\tphone_calls_df.caller_id,\n",
    "\t\tphone_calls_df.receiver_id,\n",
    "\t\tcol(\"caller_info.country_id\").alias(\"caller_country_id\"),\n",
    "\t\tcol(\"receiver_info.country_id\").alias(\"receiver_country_id\")\n",
    "\t)\n",
    ")\n",
    "result_df.show()\n",
    "\n",
    "counts_df = (result_df\n",
    "\t.agg(\n",
    "\t\tcount(\"*\").alias(\"total_calls\"),\n",
    "\t\tcount(\n",
    "\t\t\twhen(col(\"caller_country_id\") != col(\"receiver_country_id\"), 1)\n",
    "\t\t).alias(\"international_calls\")\n",
    "\t)\n",
    ")\n",
    "\n",
    "percentage_df = counts_df.select(\n",
    "\t(round(100.0 * col(\"international_calls\") / col(\"total_calls\"), 1))\n",
    "\t.alias(\"international_call_pct\")\n",
    ")\n",
    "\n",
    "# Show the percentage DataFrame\n",
    "percentage_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c95dea",
   "metadata": {},
   "source": [
    "\n",
    "36. The Bloomberg terminal is the go-to resource for financial professionals, offering convenient\n",
    "access to a wide array of financial datasets. As a Data Analyst at Bloomberg, you have access to\n",
    "historical data on stock performance.\n",
    "Currently, you're analyzing the highest and lowest open prices for each FAANG stock by month over the years.\n",
    "For each FAANG stock, display the ticker symbol, the month and year ('Mon-YYYY') with the\n",
    "corresponding highest and lowest open prices (refer to the Example Output format). Ensure that the\n",
    "results are sorted by ticker symbol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04d4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+--------------+\n",
      "|               date|ticker|  open|  high|   low| close|formatted_date|\n",
      "+-------------------+------+------+------+------+------+--------------+\n",
      "|01/31/2023 00:00:00|  AAPL|142.28|144.34| 142.7|144.29|  January-2023|\n",
      "|02/28/2023 00:00:00|  AAPL|146.83|149.08|147.05|147.41| February-2023|\n",
      "|03/31/2023 00:00:00|  AAPL|161.91| 165.0|162.44| 164.9|    March-2023|\n",
      "|04/30/2023 00:00:00|  AAPL|167.88|169.85|168.49|169.68|    April-2023|\n",
      "|05/31/2023 00:00:00|  AAPL|176.76|179.35|177.33|177.25|      May-2023|\n",
      "+-------------------+------+------+------+------+------+--------------+\n",
      "\n",
      "+------+--------------+------------+\n",
      "|ticker|formatted_date|highest_open|\n",
      "+------+--------------+------------+\n",
      "|  AAPL|      May-2023|      176.76|\n",
      "+------+--------------+------------+\n",
      "\n",
      "+------+--------------+-----------+\n",
      "|ticker|formatted_date|lowest_open|\n",
      "+------+--------------+-----------+\n",
      "|  AAPL|  January-2023|     142.28|\n",
      "+------+--------------+-----------+\n",
      "\n",
      "+------+--------------+------------+--------------+-----------+\n",
      "|ticker|formatted_date|highest_open|formatted_date|lowest_open|\n",
      "+------+--------------+------------+--------------+-----------+\n",
      "|AAPL  |May-2023      |176.76      |January-2023  |142.28     |\n",
      "+------+--------------+------------+--------------+-----------+\n",
      "\n",
      "+------+------------+------------+\n",
      "|ticker|highest_open|lowest_close|\n",
      "+------+------------+------------+\n",
      "|  AAPL|      176.76|      142.28|\n",
      "+------+------------+------------+\n",
      "\n",
      "+------+-----------+------------+----------+-----------+\n",
      "|ticker|highest_mth|highest_open|lowest_mth|lowest_open|\n",
      "+------+-----------+------------+----------+-----------+\n",
      "|  AAPL|   May-2023|      176.76|  Jan-2023|     142.28|\n",
      "+------+-----------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\"]\n",
    "\n",
    "data = [\n",
    "    (\"01/31/2023 00:00:00\", \"AAPL\", 142.28, 144.34, 142.70, 144.29),\n",
    "    (\"02/28/2023 00:00:00\", \"AAPL\", 146.83, 149.08, 147.05, 147.41),\n",
    "    (\"03/31/2023 00:00:00\", \"AAPL\", 161.91, 165.00, 162.44, 164.90),\n",
    "    (\"04/30/2023 00:00:00\", \"AAPL\", 167.88, 169.85, 168.49, 169.68),\n",
    "    (\"05/31/2023 00:00:00\", \"AAPL\", 176.76, 179.35, 177.33, 177.25),\n",
    "]\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "# Show the DataFrame\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"formatted_date\", date_format(to_date(\"date\", \"MM/dd/yyyy HH:mm:ss\"), \"MMMM-yyyy\")\n",
    ")\n",
    "df.show()\n",
    "\n",
    "window_spec_max = Window.partitionBy(\"ticker\").orderBy(col(\"open\").desc())\n",
    "window_spec_min = Window.partitionBy(\"ticker\").orderBy(\"open\")\n",
    "\n",
    "# the point here is that we create our wanted date format and then we select the rows with min and max as\n",
    "# separate df and join them as the solution\n",
    "df_with_max = (\n",
    "    df.withColumn(\"highest_open\", max(\"open\").over(Window.partitionBy(\"ticker\")))\n",
    "    .filter(col(\"open\") == col(\"highest_open\"))\n",
    "    .select(\"ticker\", \"formatted_date\", \"highest_open\")\n",
    ")\n",
    "\n",
    "df_with_min = (\n",
    "    df.withColumn(\"lowest_open\", min(\"open\").over(Window.partitionBy(\"ticker\")))\n",
    "    .filter(col(\"open\") == col(\"lowest_open\"))\n",
    "    .select(\"ticker\", \"formatted_date\", \"lowest_open\")\n",
    ")\n",
    "\n",
    "df_with_max.show()\n",
    "df_with_min.show()\n",
    "\n",
    "# Join the DataFrames to include the dates for max and min open\n",
    "final_df = df_with_max.join(df_with_min, \"ticker\", \"inner\")\n",
    "\n",
    "# Show the final result\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "\n",
    "df.groupBy(\"ticker\").agg(\n",
    "    max(\"open\").alias(\"highest_open\"), min(\"open\").alias(\"lowest_close\")\n",
    ").show()\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH highest_prices AS (\n",
    "        SELECT \n",
    "            ticker,\n",
    "            date_format(to_date(date, 'MM/dd/yyyy HH:mm:ss'), 'MMM-yyyy') AS highest_mth,\n",
    "            MAX(open) AS highest_open,\n",
    "            ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY open DESC) AS row_num\n",
    "        FROM \n",
    "            df\n",
    "        GROUP BY \n",
    "            ticker, \n",
    "            date_format(to_date(date, 'MM/dd/yyyy HH:mm:ss'), 'MMM-yyyy'),\n",
    "            open\n",
    "    ),\n",
    "    lowest_prices AS (\n",
    "        SELECT \n",
    "            ticker,\n",
    "            date_format(to_date(date, 'MM/dd/yyyy HH:mm:ss'), 'MMM-yyyy') AS lowest_mth,\n",
    "            MIN(open) AS lowest_open,\n",
    "            ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY open) AS row_num\n",
    "        FROM \n",
    "            df\n",
    "        GROUP BY \n",
    "            ticker, \n",
    "            date_format(to_date(date, 'MM/dd/yyyy HH:mm:ss'), 'MMM-yyyy'),\n",
    "            open\n",
    "    )\n",
    "    SELECT\n",
    "        highest.ticker,\n",
    "        highest.highest_mth,\n",
    "        highest.highest_open,\n",
    "        lowest.lowest_mth,\n",
    "        lowest.lowest_open\n",
    "    FROM \n",
    "        highest_prices AS highest\n",
    "    INNER JOIN \n",
    "        lowest_prices AS lowest\n",
    "    ON \n",
    "        highest.ticker = lowest.ticker\n",
    "        AND \n",
    "        highest.row_num = 1 -- Highest open price\n",
    "        AND \n",
    "        lowest.row_num = 1 -- Lowest open price\n",
    "    ORDER BY \n",
    "        highest.ticker;\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076687b",
   "metadata": {},
   "source": [
    "37. UnitedHealth Group (UHG) has a program called Advocate4Me, which allows policy holders (or, members) to call an advocate and receive support for their health care needs – whether that's claims and benefits support, drug coverage, pre- and post-authorisation, medical records, emergency assistance, or member portal services.\n",
    "Calls to the Advocate4Me call centre are classified into various categories, but some calls cannot be neatly categorised. These uncategorised calls are labeled as “n/a”, or are left empty when the support agent does not enter anything into the call category field.\n",
    "Write a query to calculate the percentage of calls that cannot be categorised. Round your answer to 1 decimal place. For example, 45.0, 48.5, 57.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a325e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------+--------------------+--------------------+------------------+\n",
      "|policy_holder_id|case_id                             |call_category       |call_date           |call_duration_secs|\n",
      "+----------------+------------------------------------+--------------------+--------------------+------------------+\n",
      "|1               |f1d012f9-9d02-4966-a968-bf6c5bc9a9fe|emergency assistance|2023-04-13T19:16:53Z|144               |\n",
      "|1               |41ce8fb6-1ddd-4f50-ac31-07bfcce6aaab|authorisation       |2023-05-25T09:09:30Z|815               |\n",
      "|2               |9b1af84b-eedb-4c21-9730-6f099cc2cc5e|n/a                 |2023-01-26T01:21:27Z|992               |\n",
      "|2               |8471a3d4-6fc7-4bb2-9fc7-4583e3638a9e|emergency assistance|2023-03-09T10:58:54Z|128               |\n",
      "|2               |38208fae-bad0-49bf-99aa-7842ba2e37bc|NULL                |2023-06-05T07:35:43Z|619               |\n",
      "+----------------+------------------------------------+--------------------+--------------------+------------------+\n",
      "\n",
      "+----------------------+\n",
      "|uncategorised_call_pct|\n",
      "+----------------------+\n",
      "|                  40.0|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|uncategorised_call_pct|\n",
      "+----------------------+\n",
      "|                  40.0|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"policy_holder_id\", \"case_id\", \"call_category\", \"call_date\", \"call_duration_secs\"]\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    (\n",
    "        1,\n",
    "        \"f1d012f9-9d02-4966-a968-bf6c5bc9a9fe\",\n",
    "        \"emergency assistance\",\n",
    "        \"2023-04-13T19:16:53Z\",\n",
    "        144,\n",
    "    ),\n",
    "    (\n",
    "        1,\n",
    "        \"41ce8fb6-1ddd-4f50-ac31-07bfcce6aaab\",\n",
    "        \"authorisation\",\n",
    "        \"2023-05-25T09:09:30Z\",\n",
    "        815,\n",
    "    ),\n",
    "    (2, \"9b1af84b-eedb-4c21-9730-6f099cc2cc5e\", \"n/a\", \"2023-01-26T01:21:27Z\", 992),\n",
    "    (\n",
    "        2,\n",
    "        \"8471a3d4-6fc7-4bb2-9fc7-4583e3638a9e\",\n",
    "        \"emergency assistance\",\n",
    "        \"2023-03-09T10:58:54Z\",\n",
    "        128,\n",
    "    ),\n",
    "    (\n",
    "        2,\n",
    "        \"38208fae-bad0-49bf-99aa-7842ba2e37bc\",\n",
    "        None,\n",
    "        \"2023-06-05T07:35:43Z\",\n",
    "        619,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    with uncategorised_callers as(\n",
    "        SELECT \n",
    "            COUNT(*) AS count\n",
    "        FROM \n",
    "            df\n",
    "        WHERE \n",
    "            call_category IS NULL\n",
    "            OR call_category = 'n/a'\n",
    "            OR call_category = ''         \n",
    "    )\n",
    "    SELECT \n",
    "        ROUND(100.0 * count \n",
    "            / (SELECT COUNT(*) FROM df), 1) AS uncategorised_call_pct\n",
    "    FROM \n",
    "        uncategorised_callers;\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "total_count_df = df.select(count(\"*\").alias(\"total_count\"))\n",
    "\n",
    "# Step 2: Calculate count of uncategorised calls\n",
    "uncategorised_count_df = df.filter(\n",
    "    (col(\"call_category\").isNull())\n",
    "    | (col(\"call_category\") == \"n/a\")\n",
    "    | (col(\"call_category\") == \"\")\n",
    ").select(count(\"*\").alias(\"uncategorised_count\"))\n",
    "\n",
    "# Step 3: Join the two counts and compute percentage\n",
    "percentage_df = uncategorised_count_df.crossJoin(total_count_df).select(\n",
    "    round(100.0 * col(\"uncategorised_count\") / col(\"total_count\"), 1).alias(\n",
    "        \"uncategorised_call_pct\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 4: Show the result\n",
    "percentage_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dac8a6",
   "metadata": {},
   "source": [
    "39. Zomato is a leading online food delivery service that connects users with various restaurants and cuisines, allowing them to browse menus, place orders, and get meals delivered to their doorsteps.\n",
    "Recently, Zomato encountered an issue with their delivery system. Due to an error in the delivery driver instructions, each item's order was swapped with the item in the subsequent row. As a data analyst, you're asked to correct this swapping error and return the proper pairing of order ID and item.\n",
    "If the last item has an odd order ID, it should remain as the last item in the corrected data. For example, if the last item is Order ID 7 Tandoori Chicken, then it should remain as Order ID 7 in the corrected data.\n",
    "In the results, return the correct pairs of order IDs and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fbec26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|order_id|            item|\n",
      "+--------+----------------+\n",
      "|       1|       Chow Mein|\n",
      "|       2|           Pizza|\n",
      "|       3|        Pad Thai|\n",
      "|       4|  Butter Chicken|\n",
      "|       5|        Eggrolls|\n",
      "|       6|          Burger|\n",
      "|       7|Tandoori Chicken|\n",
      "+--------+----------------+\n",
      "\n",
      "+--------+----------------+\n",
      "|order_id|            item|\n",
      "+--------+----------------+\n",
      "|       1|           Pizza|\n",
      "|       2|       Chow Mein|\n",
      "|       3|  Butter Chicken|\n",
      "|       4|        Pad Thai|\n",
      "|       5|          Burger|\n",
      "|       6|        Eggrolls|\n",
      "|       7|Tandoori Chicken|\n",
      "+--------+----------------+\n",
      "\n",
      "+--------+----------------+\n",
      "|order_id|            item|\n",
      "+--------+----------------+\n",
      "|       1|           Pizza|\n",
      "|       2|       Chow Mein|\n",
      "|       3|  Butter Chicken|\n",
      "|       4|        Pad Thai|\n",
      "|       5|          Burger|\n",
      "|       6|        Eggrolls|\n",
      "|       7|Tandoori Chicken|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Chow Mein\"),\n",
    "    (2, \"Pizza\"),\n",
    "    (3, \"Pad Thai\"),\n",
    "    (4, \"Butter Chicken\"),\n",
    "    (5, \"Eggrolls\"),\n",
    "    (6, \"Burger\"),\n",
    "    (7, \"Tandoori Chicken\"),\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"item\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df.show()\n",
    "window_spec = Window.orderBy(\"order_id\")\n",
    "lead_lag_df = (\n",
    "    df.withColumn(\"leading\", lead(\"item\", 1).over(window_spec))\n",
    "    .withColumn(\"lagging\", lag(\"item\", 1).over(window_spec))\n",
    "    .withColumn(\"is_last\", lead(\"order_id\", 1).over(window_spec).isNull())\n",
    ")\n",
    "res = lead_lag_df.withColumn(\n",
    "    \"item\",\n",
    "    when(\n",
    "        col(\"is_last\") & (col(\"order_id\") % 2 != 0), col(\"item\")\n",
    "    ).otherwise(  \n",
    "        when(\n",
    "            col(\"order_id\") % 2 == 0, col(\"lagging\")\n",
    "        ).otherwise(  \n",
    "            col(\"leading\")\n",
    "        )  \n",
    "    ),\n",
    ").select(\"order_id\", \"item\")\n",
    "res.show()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        order_id,\n",
    "        case\n",
    "            when lead(order_id,1) over(order by order_id) is null and order_id % 2 !=0 then item\n",
    "            when order_id % 2=0 then lag(item, 1) over( order by order_id)\n",
    "            else lead(item ,1)over(order by order_id)\n",
    "        end as item\n",
    "    from\n",
    "        df\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a864ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+-------------+----------+\n",
      "|employee_id|            name|salary|department_id|manager_id|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "|          1|   Emma Thompson|  3800|            1|         6|\n",
      "|          2|Daniel Rodriguez|  2230|            1|         7|\n",
      "|          3|    Olivia Smith|  7000|            2|         8|\n",
      "|          4|   James Johnson|  5000|            2|         8|\n",
      "|          5| Sophia Martinez|  1750|            3|        11|\n",
      "|          6|   Michael Brown|  4500|            3|        11|\n",
      "|          7| Isabella Garcia|  6000|            4|        12|\n",
      "|          8|     Ethan Davis|  3000|            4|        12|\n",
      "|          9|      Ava Wilson|  7500|            5|        13|\n",
      "|         10|   Alexander Lee|  2900|            5|        13|\n",
      "+-----------+----------------+------+-------------+----------+\n",
      "\n",
      "+---------+-----------+------+-------------------+\n",
      "|salary_id|employee_id|amount|       payment_date|\n",
      "+---------+-----------+------+-------------------+\n",
      "|        1|          1|  3800|2024-03-31 00:00:00|\n",
      "|        2|          2|  2230|2024-03-31 00:00:00|\n",
      "|        3|          3|  7000|2024-03-31 00:00:00|\n",
      "|        4|          4|  6800|2024-03-31 00:00:00|\n",
      "|        5|          5|  1750|2024-03-31 00:00:00|\n",
      "|        6|          6|  4500|2024-03-31 00:00:00|\n",
      "|        7|          7|  6000|2024-03-31 00:00:00|\n",
      "|        8|          8|  3000|2024-03-31 00:00:00|\n",
      "|        9|          9|  7500|2024-03-31 00:00:00|\n",
      "|       10|         10|  2900|2024-03-31 00:00:00|\n",
      "|       11|          1|  3800|2024-04-30 00:00:00|\n",
      "|       12|          2|  2230|2024-04-30 00:00:00|\n",
      "|       13|          3|  7000|2024-04-30 00:00:00|\n",
      "|       14|          4|  6800|2024-04-30 00:00:00|\n",
      "|       15|          5|  1750|2024-04-30 00:00:00|\n",
      "|       16|          6|  4500|2024-04-30 00:00:00|\n",
      "|       17|          7|  6000|2024-04-30 00:00:00|\n",
      "|       18|          8|  3000|2024-04-30 00:00:00|\n",
      "|       19|          9|  7500|2024-04-30 00:00:00|\n",
      "|       20|         10|  2900|2024-04-30 00:00:00|\n",
      "+---------+-----------+------+-------------------+\n",
      "\n",
      "+-------------+------------+----------+\n",
      "|department_id|payment_date|comparison|\n",
      "+-------------+------------+----------+\n",
      "|            1|     03/2024|     lower|\n",
      "|            3|     03/2024|     lower|\n",
      "|            2|     03/2024|    higher|\n",
      "|            5|     03/2024|    higher|\n",
      "|            4|     03/2024|      same|\n",
      "+-------------+------------+----------+\n",
      "\n",
      "+-------------+------------+----------+\n",
      "|department_id|payment_date|comparison|\n",
      "+-------------+------------+----------+\n",
      "|            1|     03/2024|     lower|\n",
      "|            3|     03/2024|     lower|\n",
      "|            2|     03/2024|    higher|\n",
      "|            5|     03/2024|    higher|\n",
      "|            4|     03/2024|      same|\n",
      "+-------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (1, \"Emma Thompson\", 3800, 1, 6),\n",
    "    (2, \"Daniel Rodriguez\", 2230, 1, 7),\n",
    "    (3, \"Olivia Smith\", 7000, 2, 8),\n",
    "    (4, \"James Johnson\", 5000, 2, 8),\n",
    "    (5, \"Sophia Martinez\", 1750, 3, 11),\n",
    "    (6, \"Michael Brown\", 4500, 3, 11),\n",
    "    (7, \"Isabella Garcia\", 6000, 4, 12),\n",
    "    (8, \"Ethan Davis\", 3000, 4, 12),\n",
    "    (9, \"Ava Wilson\", 7500, 5, 13),\n",
    "    (10, \"Alexander Lee\", 2900, 5, 13),\n",
    "]\n",
    "\n",
    "employee_columns = [\"employee_id\", \"name\", \"salary\", \"department_id\", \"manager_id\"]\n",
    "\n",
    "# Expanded salary data\n",
    "salary_data = [\n",
    "    (1, 1, 3800, \"03/31/2024 00:00:00\"),\n",
    "    (2, 2, 2230, \"03/31/2024 00:00:00\"),\n",
    "    (3, 3, 7000, \"03/31/2024 00:00:00\"),\n",
    "    (4, 4, 6800, \"03/31/2024 00:00:00\"),\n",
    "    (5, 5, 1750, \"03/31/2024 00:00:00\"),\n",
    "    (6, 6, 4500, \"03/31/2024 00:00:00\"),\n",
    "    (7, 7, 6000, \"03/31/2024 00:00:00\"),\n",
    "    (8, 8, 3000, \"03/31/2024 00:00:00\"),\n",
    "    (9, 9, 7500, \"03/31/2024 00:00:00\"),\n",
    "    (10, 10, 2900, \"03/31/2024 00:00:00\"),\n",
    "    (11, 1, 3800, \"04/30/2024 00:00:00\"),\n",
    "    (12, 2, 2230, \"04/30/2024 00:00:00\"),\n",
    "    (13, 3, 7000, \"04/30/2024 00:00:00\"),\n",
    "    (14, 4, 6800, \"04/30/2024 00:00:00\"),\n",
    "    (15, 5, 1750, \"04/30/2024 00:00:00\"),\n",
    "    (16, 6, 4500, \"04/30/2024 00:00:00\"),\n",
    "    (17, 7, 6000, \"04/30/2024 00:00:00\"),\n",
    "    (18, 8, 3000, \"04/30/2024 00:00:00\"),\n",
    "    (19, 9, 7500, \"04/30/2024 00:00:00\"),\n",
    "    (20, 10, 2900, \"04/30/2024 00:00:00\"),\n",
    "]\n",
    "salary_columns = [\"salary_id\", \"employee_id\", \"amount\", \"payment_date\"]\n",
    "\n",
    "# Create DataFrames\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_columns)\n",
    "salary_df = spark.createDataFrame(salary_data, schema=salary_columns)\n",
    "\n",
    "salary_df = salary_df.withColumn(\n",
    "    \"payment_date\",\n",
    "    to_timestamp(\n",
    "        \"payment_date\", \"MM/dd/yyyy HH:mm:ss\"\n",
    "    ), \n",
    ")\n",
    "employee_df.show()\n",
    "salary_df.show()\n",
    "\n",
    "employee_df.createOrReplaceTempView(\"employee\")\n",
    "salary_df.createOrReplaceTempView(\"salary\")\n",
    "\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH company_avg AS ( \n",
    "        SELECT \n",
    "            payment_date,\n",
    "            AVG(amount) AS co_avg_salary\n",
    "        FROM \n",
    "            salary\n",
    "        WHERE \n",
    "            payment_date = to_timestamp('03/31/2024 00:00:00', 'MM/dd/yyyy HH:mm:ss')\n",
    "        GROUP BY \n",
    "            payment_date\n",
    "    ), \n",
    "    dept_avg AS ( \n",
    "        SELECT\n",
    "            e.department_id,\n",
    "            s.payment_date,\n",
    "            AVG(s.amount) AS dept_avg_salary\n",
    "        FROM \n",
    "            salary AS s\n",
    "        INNER JOIN \n",
    "            employee AS e\n",
    "        ON \n",
    "            s.employee_id = e.employee_id\n",
    "        WHERE \n",
    "            s.payment_date = to_timestamp('03/31/2024 00:00:00', 'MM/dd/yyyy HH:mm:ss')\n",
    "        GROUP BY \n",
    "            e.department_id, s.payment_date\n",
    "    )\n",
    "    SELECT\n",
    "        d.department_id,\n",
    "        date_format(d.payment_date, 'MM/yyyy') AS payment_date, -- Update the pattern\n",
    "        CASE  \n",
    "            WHEN d.dept_avg_salary > c.co_avg_salary+100 THEN 'higher'\n",
    "            WHEN d.dept_avg_salary < c.co_avg_salary-100 THEN 'lower'\n",
    "            ELSE 'same'\n",
    "        END AS comparison\n",
    "    FROM \n",
    "        dept_avg AS d\n",
    "    INNER JOIN \n",
    "        company_avg AS c\n",
    "    ON \n",
    "        d.payment_date = c.payment_date;\n",
    "\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "\n",
    "company_avg_salary_df = (\n",
    "    salary_df\n",
    "    .filter(col(\"payment_date\")=='2024-03-31 00:00:00')\n",
    "    .groupBy(\"payment_date\")\n",
    "    .agg(avg(\"amount\").alias(\"company_avg_salary\"))\n",
    ")\n",
    "# company_avg_salary_df.show()\n",
    "dept_avg_salary = (\n",
    "    salary_df\n",
    "    .filter(col(\"payment_date\")=='2024-03-31 00:00:00')\n",
    "    .join(employee_df, \"employee_id\", \"inner\")\n",
    "    .groupBy(employee_df.department_id, salary_df.payment_date)\n",
    "    .agg(avg(col(\"amount\")).alias(\"avg_salary_by_dept\")) \n",
    "    .select(employee_df.department_id, salary_df.payment_date, \"avg_salary_by_dept\")\n",
    ")\n",
    "# dept_avg_salary.show()\n",
    "res_df =  (\n",
    "    dept_avg_salary\n",
    "    .join(company_avg_salary_df, \"payment_date\", \"inner\")\n",
    "    .withColumn(\n",
    "        \"comparison\", \n",
    "        when(\n",
    "            col(\"avg_salary_by_dept\") > col(\"company_avg_salary\")+100, \n",
    "            'higher'\n",
    "        ).otherwise(\n",
    "            when(\n",
    "                col(\"avg_salary_by_dept\") < col(\"company_avg_salary\")-100, 'lower'\n",
    "            ).otherwise(\"same\")\n",
    "        )\n",
    "    ).select(\n",
    "        \"department_id\", \n",
    "        date_format(\"payment_date\", \"MM/yyyy\").alias(\"payment_date\"), \n",
    "        \"comparison\"\n",
    "    )\n",
    ")\n",
    "res_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "594a6fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+-------------------+\n",
      "|user_id|event_id|event_type|         event_date|\n",
      "+-------+--------+----------+-------------------+\n",
      "|    445|    7765|   sign-in|2022-05-31 12:00:00|\n",
      "|    742|    6458|   sign-in|2022-06-03 12:00:00|\n",
      "|    445|    3634|      like|2022-06-05 12:00:00|\n",
      "|    742|    1374|   comment|2022-06-05 12:00:00|\n",
      "|    648|    3124|      like|2022-06-18 12:00:00|\n",
      "+-------+--------+----------+-------------------+\n",
      "\n",
      "curr_month\n",
      "+-------+--------+----------+-------------------+\n",
      "|user_id|event_id|event_type|         event_date|\n",
      "+-------+--------+----------+-------------------+\n",
      "|    742|    6458|   sign-in|2022-06-03 12:00:00|\n",
      "|    445|    3634|      like|2022-06-05 12:00:00|\n",
      "|    742|    1374|   comment|2022-06-05 12:00:00|\n",
      "|    648|    3124|      like|2022-06-18 12:00:00|\n",
      "+-------+--------+----------+-------------------+\n",
      "\n",
      "last_month\n",
      "+-------+--------+----------+-------------------+\n",
      "|user_id|event_id|event_type|         event_date|\n",
      "+-------+--------+----------+-------------------+\n",
      "|    445|    7765|   sign-in|2022-05-31 12:00:00|\n",
      "+-------+--------+----------+-------------------+\n",
      "\n",
      "monthly_active_users\n",
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|    445|\n",
      "+-------+\n",
      "\n",
      "monthly_active_users_count\n",
      "+---+--------------------+\n",
      "|mth|monthly_active_users|\n",
      "+---+--------------------+\n",
      "|  6|                   1|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+--------------------+\n",
      "|mth|monthly_active_users|\n",
      "+---+--------------------+\n",
      "|  6|                   1|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (445, 7765, \"sign-in\", \"2022-05-31 12:00:00\"),\n",
    "    (742, 6458, \"sign-in\", \"2022-06-03 12:00:00\"),\n",
    "    (445, 3634, \"like\", \"2022-06-05 12:00:00\"),\n",
    "    (742, 1374, \"comment\", \"2022-06-05 12:00:00\"),\n",
    "    (648, 3124, \"like\", \"2022-06-18 12:00:00\"),\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"user_id\", \"event_id\", \"event_type\", \"event_date\"]\n",
    "\n",
    "# Create the DataFrame with inferred schema\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"event_date\", to_timestamp(\"event_date\"))\n",
    "df.createOrReplaceTempView(\"user_actions\")\n",
    "df.show()\n",
    "\n",
    "curr_month = df.filter((month(\"event_date\") == 6) & (year(\"event_date\") == 2022))\n",
    "last_month = df.filter((month(\"event_date\") == 5) & (year(\"event_date\") == 2022))\n",
    "print(\"curr_month\")\n",
    "curr_month.show()\n",
    "print(\"last_month\")\n",
    "last_month.show()\n",
    "\n",
    "monthly_active_users = (\n",
    "    curr_month.join(last_month, on=\"user_id\", how=\"inner\")\n",
    "    .select(curr_month.user_id)\n",
    "    .distinct()\n",
    ")\n",
    "print(\"monthly_active_users\")\n",
    "monthly_active_users.show()\n",
    "\n",
    "monthly_active_users_count = monthly_active_users.groupBy().agg(\n",
    "    lit(6).alias(\"mth\"),  \n",
    "    count(\"user_id\").alias(\"monthly_active_users\"),\n",
    ")\n",
    "print(\"monthly_active_users_count\")\n",
    "monthly_active_users_count.show()\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(MONTH FROM curr_month.event_date) AS mth,\n",
    "        COUNT(DISTINCT curr_month.user_id) AS monthly_active_users\n",
    "    FROM \n",
    "        user_actions AS curr_month\n",
    "    WHERE \n",
    "        EXISTS (\n",
    "            SELECT \n",
    "                last_month.user_id\n",
    "            FROM \n",
    "                user_actions AS last_month\n",
    "            WHERE \n",
    "                last_month.user_id = curr_month.user_id\n",
    "                AND \n",
    "                MONTH(last_month.event_date) = MONTH(add_months(curr_month.event_date, -1))\n",
    "                AND \n",
    "                YEAR(last_month.event_date) = YEAR(add_months(curr_month.event_date, -1))\n",
    "        )\n",
    "        AND \n",
    "        EXTRACT(MONTH FROM curr_month.event_date) = 6\n",
    "        AND \n",
    "        EXTRACT(YEAR FROM curr_month.event_date) = 2022\n",
    "        GROUP BY \n",
    "            EXTRACT(MONTH FROM curr_month.event_date);\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26853c10",
   "metadata": {},
   "source": [
    "input_df1\n",
    "|Emp_id|Name|Department_id|Hire_date|Salary|Age|\n",
    "|:-----:|:-----:|:-----:|:-------:|:-------:|:-----:|\n",
    "|     1|  Alice|          101|2020-01-15| 70000| 29|\n",
    "|     2|    Bob|          102|2018-06-23| 60000| 35|\n",
    "|     3|Charlie|          101|2019-07-01| 90000| 42|\n",
    "|     4|  David|          103|2021-03-12| 50000| 24|\n",
    "|     5|    Eve|          102|2020-09-10| 75000| 30|\n",
    "\n",
    "input_df2\n",
    "\n",
    "|Department_id|Department_name|     Location|\n",
    "|:-------|:---:|--------:|\n",
    "|          101|    Engineering|     New York|\n",
    "|          102|      Marketing|San Francisco|\n",
    "|          103|          Sales|      Chicago|\n",
    "\n",
    "result_df\n",
    "\n",
    "Note: \n",
    "find the average salary  and the max_age by department\n",
    "|department_name|avg_salary|max_age|\n",
    "|:-------|:---:|--------:|\n",
    "|    Engineering|   80000.0|     42|\n",
    "|      Marketing|   67500.0|     35|\n",
    "|          Sales|   50000.0|     24|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efac0739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+----------+------+---+\n",
      "|emp_id|   name|department_id| hire_date|salary|age|\n",
      "+------+-------+-------------+----------+------+---+\n",
      "|     1|  Alice|          101|2020-01-15| 70000| 29|\n",
      "|     2|    Bob|          102|2018-06-23| 60000| 35|\n",
      "|     3|Charlie|          101|2019-07-01| 90000| 42|\n",
      "|     4|  David|          103|2021-03-12| 50000| 24|\n",
      "|     5|    Eve|          102|2020-09-10| 75000| 30|\n",
      "+------+-------+-------------+----------+------+---+\n",
      "\n",
      "+-------------+---------------+-------------+\n",
      "|department_id|department_name|     location|\n",
      "+-------------+---------------+-------------+\n",
      "|          101|    Engineering|     New York|\n",
      "|          102|      Marketing|San Francisco|\n",
      "|          103|          Sales|      Chicago|\n",
      "+-------------+---------------+-------------+\n",
      "\n",
      "+---------------+----------+-------+\n",
      "|department_name|avg_salary|max_age|\n",
      "+---------------+----------+-------+\n",
      "|    Engineering|   80000.0|     42|\n",
      "|      Marketing|   67500.0|     35|\n",
      "|          Sales|   50000.0|     24|\n",
      "+---------------+----------+-------+\n",
      "\n",
      "+---------------+-----------+--------+\n",
      "|department_name|avg(salary)|max(age)|\n",
      "+---------------+-----------+--------+\n",
      "|    Engineering|    80000.0|      42|\n",
      "|      Marketing|    67500.0|      35|\n",
      "|          Sales|    50000.0|      24|\n",
      "+---------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [(1, 'Alice', 101, '2020-01-15', 70000, 29),\n",
    "\t\t\t (2, 'Bob', 102, '2018-06-23', 60000, 35),\n",
    "\t\t\t (3, 'Charlie', 101, '2019-07-01', 90000, 42),\n",
    "\t\t\t (4, 'David', 103, '2021-03-12', 50000, 24),\n",
    "\t\t\t (5, 'Eve', 102, '2020-09-10', 75000, 30)]\n",
    "\n",
    "departments = [(101, 'Engineering', 'New York'),\n",
    "\t\t\t   (102, 'Marketing', 'San Francisco'),\n",
    "\t\t\t   (103, 'Sales', 'Chicago')]\n",
    "# Creating DataFrames\n",
    "df_employees = spark.createDataFrame(\n",
    "\t\t\t\t\t\t\temployees,\n",
    "\t\t\t\t\t\t\t['emp_id', 'name', 'department_id', 'hire_date', 'salary', 'age']\n",
    "\t\t\t\t\t)\n",
    "df_departments = spark.createDataFrame(departments, ['department_id', 'department_name', 'location'])\n",
    "df_employees.show()\n",
    "df_departments.show()\n",
    "\n",
    "joined_df = (df_departments\n",
    "\t\t\t .join(df_employees, \"department_id\", \"left\")\n",
    "\t\t\t .groupby(\"department_name\")\n",
    "\t\t\t .agg(\n",
    "\t\t\t\tavg(\"salary\").alias(\"avg_salary\"),\n",
    "\t\t\t\tmax(\"age\").alias(\"max_age\")\n",
    "\t\t\t ).orderBy(\"department_name\"))  #.select(\"department_name\", \"avg_salary\", \"max_age\")\n",
    "joined_df.show()\n",
    "#note: with spark.sql\n",
    "df_employees.createOrReplaceTempView(\"employees\")\n",
    "df_departments.createOrReplaceTempView(\"departments\")\n",
    "spark.sql(\"\"\"\n",
    "\tselect \n",
    "\t\td.department_name, avg(e.salary), max(e.age)\n",
    "\tfrom departments d\n",
    "\t\tleft join employees e\n",
    "\t\ton d.department_id = e.department_id\n",
    "\tgroup by department_name\n",
    "\torder by department_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5957492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656efce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed9aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39ea1d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If you don’t take risks, you can’t create a future.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def motivate_me():\n",
    "    li = [\n",
    "        \"You have been defeated today... \\nWhat will you become tomorrow\",\n",
    "        \"Being weak in anything is a disadvantage.... \\nbut not a sign of INCOMPETENCE\",\n",
    "        \"Habits become Second Nature....\",\n",
    "        \"Being Good means Being Free\",\n",
    "        \"The World is utterly unfair. But at the same time.. \\n...Its Ruthlessly Fair\",\n",
    "        \"Because people don't have wings... We look for ways to fly.\",\n",
    "        \"The most meaningless thing is Do something just to DO it.\",\n",
    "        \"No Matter How Hard or Easy the skill...\\nGetting Good at something is Fun\",\n",
    "        \"If you Really get Good.... somebody who's even better will come and find you\",\n",
    "        \"He who would climb the ladder must begin at the bottom.\",\n",
    "        \"We aren't limited.... to only one way of being Great!\",\n",
    "        \"If you don’t take risks, you can’t create a future.\",\n",
    "        \"The future belongs to those who believe in the beauty of their dreams.\",\n",
    "        # \"If you can, cut your desires lose they'll drag you down.\",\n",
    "        \"Iam only Here... Because My Family and Friends Brought me Here....\",\n",
    "    ]\n",
    "    print(random.choice(li))\n",
    "\n",
    "\n",
    "# print(list[random.randint(0,len(list-1))])\n",
    "\n",
    "print()\n",
    "motivate_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845694c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183731f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ca799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff9f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a84b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a07731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88810a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
